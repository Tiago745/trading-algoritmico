{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f8d5a2",
   "metadata": {},
   "source": [
    "# Modelo MLP para Previsão de Preços de Criptomoedas (Refatorado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6b4d3",
   "metadata": {},
   "source": [
    "## Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0c2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/david.leati/AppData/Local/Microsoft/WindowsApps/python3.12.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, Normalizer, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, PReLU, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, AdamW, Lion, RMSprop\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0a1f7",
   "metadata": {},
   "source": [
    "## Funções do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dados(caminho_arquivo, ultimas_linhas=100000):\n",
    "    \"\"\"\n",
    "    PASSO 1: Carrega e prepara os dados.\n",
    "    Objetivo: Ler o arquivo CSV e garantir que ele esteja em ordem.\n",
    "    \"\"\"\n",
    "    # Carrega os dados do arquivo CSV\n",
    "    df = pd.read_csv(caminho_arquivo)\n",
    "\n",
    "    # Evita o PC de explodir com muitos dados\n",
    "    df = df.iloc[-ultimas_linhas:]\n",
    "\n",
    "    # Garante que a coluna 'timestamp' seja do tipo data e ordena os dados\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Remove colunas com nomes \"Unnamed\".\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    print(f\"Dados carregados: {df.shape[0]} registros.\")\n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_features(df, df_btc=None, df_eth=None):\n",
    "    \"\"\"\n",
    "    PASSO 2: Preparar os Dados para o Modelo.\n",
    "    Objetivo: Transformar os dados brutos em um formato que a rede neural entenda (features e target).\n",
    "    \"\"\"\n",
    "    # --- Calculo dos Indicadores ---\n",
    "    df = df.copy()\n",
    "\n",
    "    if df_btc is not None:\n",
    "        btc_features = pd.DataFrame(index=df_btc.index)\n",
    "        btc_features['btc_close_pct'] = np.log1p(df_btc['close'].pct_change())\n",
    "        btc_features['btc_volume_pct'] = np.log1p(df_btc['volume'].pct_change())\n",
    "        btc_features['btc_price_trend'] = df_btc['close'].pct_change().rolling(10).mean()\n",
    "        \n",
    "        df = df.join(btc_features)\n",
    "        df['btc_correlation'] = df['close'].rolling(10).corr(df_btc['close'])\n",
    "\n",
    "    if df_eth is not None:\n",
    "        eth_features = pd.DataFrame(index=df_eth.index)\n",
    "        eth_features['eth_close_pct'] = np.log1p(df_eth['close'].pct_change())\n",
    "        eth_features['eth_volume_pct'] = np.log1p(df_eth['volume'].pct_change())\n",
    "        eth_features['eth_price_trend'] = df_eth['close'].pct_change().rolling(10).mean()\n",
    "\n",
    "        df = df.join(eth_features)\n",
    "        df['eth_correlation'] = df['close'].rolling(10).corr(df_eth['close'])\n",
    "\n",
    "    # --- 1. Variações e transformações básicas ---\n",
    "    df['close_pct'] = np.log1p(df['close'].pct_change())\n",
    "    df['volume_pct'] = np.log1p(df['volume'].pct_change())\n",
    "\n",
    "    # --- 2. Médias móveis e suas diferenças ---\n",
    "    df['sma_10'] = df['close'].rolling(10).mean()\n",
    "    df['sma_30'] = df['close'].rolling(30).mean()\n",
    "    df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
    "    df['ema_30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
    "    df['sma_10_vs_sma_30'] = df['sma_10'] - df['sma_30']\n",
    "    df['price_vs_sma30'] = df['close'] - df['sma_30']\n",
    "\n",
    "    # --- 3. Indicadores técnicos comuns ---\n",
    "    def calc_rsi(series, period):\n",
    "        delta = series.diff()\n",
    "        gain = delta.clip(lower=0).rolling(period).mean()\n",
    "        loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        return 100 - 100 / (1 + rs)\n",
    "\n",
    "    for p in [7, 14, 21]:\n",
    "        df[f'rsi_{p}'] = calc_rsi(df['close'], p)\n",
    "\n",
    "    def macd(series, short=12, long=26, signal=9):\n",
    "        exp1 = series.ewm(span=short, adjust=False).mean()\n",
    "        exp2 = series.ewm(span=long, adjust=False).mean()\n",
    "        macd_line = exp1 - exp2\n",
    "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "        return macd_line, signal_line\n",
    "\n",
    "    df['macd'], df['macd_signal'] = macd(df['close'])\n",
    "    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
    "\n",
    "    def bollinger(series, window=20, std_dev=2):\n",
    "        ma = series.rolling(window).mean()\n",
    "        std = series.rolling(window).std()\n",
    "        upper = ma + std_dev * std\n",
    "        lower = ma - std_dev * std\n",
    "        return upper, lower\n",
    "\n",
    "    df['bb_upper'], df['bb_lower'] = bollinger(df['close'])\n",
    "    df['bb_upper_diff'] = df['bb_upper'] - df['close']\n",
    "    df['bb_lower_diff'] = df['close'] - df['bb_lower']\n",
    "\n",
    "    def atr(df, window=14):\n",
    "        tr = pd.concat([\n",
    "            df['high'] - df['low'],\n",
    "            abs(df['high'] - df['close'].shift()),\n",
    "            abs(df['low'] - df['close'].shift())\n",
    "        ], axis=1).max(axis=1)\n",
    "        return tr.rolling(window).mean()\n",
    "\n",
    "    df['atr_14'] = atr(df)\n",
    "\n",
    "    # --- 4. Osciladores ---\n",
    "    def stochastic(df, k_window=14, d_window=3):\n",
    "        low_min = df['low'].rolling(k_window).min()\n",
    "        high_max = df['high'].rolling(k_window).max()\n",
    "        k = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
    "        d = k.rolling(d_window).mean()\n",
    "        return k, d\n",
    "\n",
    "    df['so_k'], df['so_d'] = stochastic(df)\n",
    "\n",
    "    def williams_r(df, window=14):\n",
    "        low_min = df['low'].rolling(window).min()\n",
    "        high_max = df['high'].rolling(window).max()\n",
    "        return -100 * (high_max - df['close']) / (high_max - low_min + 1e-10)\n",
    "\n",
    "    df['wpr_14'] = williams_r(df)\n",
    "\n",
    "    # --- 5. Indicadores de volume ---\n",
    "    # OTIMIZAÇÃO: Lógica do OBV vetorizada para muito mais performance\n",
    "    price_direction = np.sign(df['close'].diff()).fillna(0)\n",
    "    df['obv'] = (price_direction * df['volume']).cumsum()\n",
    "\n",
    "    df['volume_trend'] = df['volume'].pct_change().rolling(10).mean()\n",
    "    df['price_trend'] = df['close'].pct_change().rolling(10).mean()\n",
    "\n",
    "    # --- 6. Volatilidade e desvio padrão ---\n",
    "    for w in [10, 20, 40]:\n",
    "        df[f'std_{w}'] = df['close'].rolling(w).std()\n",
    "        df[f'volatility_{w}'] = df['close'].pct_change().rolling(w).std()\n",
    "\n",
    "    # --- 7. Lags de indicadores principais ---\n",
    "    cols_to_lag = [\n",
    "        'close_pct', 'rsi_7', 'rsi_14', 'rsi_21', 'macd', 'macd_signal',\n",
    "        'btc_close_pct', 'btc_volume_pct', 'btc_price_trend', 'btc_correlation',\n",
    "        'eth_close_pct', 'eth_volume_pct', 'eth_price_trend', 'eth_correlation'\n",
    "    ]\n",
    "\n",
    "    for col in cols_to_lag:\n",
    "        if col in df.columns:  # Apenas cria o lag se a coluna existir no df final\n",
    "            df[f'lag_{col}_1'] = df[col].shift(1)\n",
    "            df[f'lag_{col}_2'] = df[col].shift(2)\n",
    "\n",
    "    # --- 8. Finalização ---\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def definir_features_e_alvo(df):\n",
    "    \"\"\"Define as colunas de features e a coluna alvo, e retorna os dados brutos X e y.\"\"\"\n",
    "    FEATURE_COLUMNS = [\n",
    "    # --- Features de Mercado Externo ---\n",
    "    # Essencial para capturar a tendência geral do mercado cripto.\n",
    "    'btc_close_pct',\n",
    "    'eth_close_pct',\n",
    "    'btc_price_trend',\n",
    "    'eth_price_trend',\n",
    "\n",
    "    # --- Features Principais do Ativo (DataFrame Principal) ---\n",
    "    # As variações de preço e volume são a base da predição.\n",
    "    'close_pct',\n",
    "    'volume_pct',\n",
    "    'price_trend',\n",
    "\n",
    "\n",
    "    # --- Indicadores de Tendência e Força ---\n",
    "    # Medem a direção e a força do movimento de preços.\n",
    "    'sma_10_vs_sma_30', # Cruzamento de médias, indica mudança de tendência\n",
    "    'price_vs_sma30',   # Distância do preço à média, indica sobre-extensão\n",
    "    'macd_diff',        # Histograma MACD, um dos sinais de momentum mais fortes\n",
    "\n",
    "    # --- Indicadores de Momento (Osciladores) ---\n",
    "    # Identificam condições de sobrecompra e sobrevenda.\n",
    "    'rsi_14',           # Índice de Força Relativa (período padrão)\n",
    "    'so_d',             # Linha de sinal do Estocástico, mais suave que %K\n",
    "\n",
    "    # --- Indicadores de Volatilidade ---\n",
    "    # Medem a magnitude das variações de preço.\n",
    "    'atr_14',           # Average True Range, mede a volatilidade real\n",
    "    'bb_upper_diff',    # Distância do preço à banda superior de Bollinger\n",
    "    'bb_lower_diff',    # Distância do preço à banda inferior de Bollinger\n",
    "\n",
    "    # --- Indicador de Volume ---\n",
    "    # Confirma a força da tendência com base no fluxo de volume.\n",
    "    # 'obv',              # On-Balance Volume (comentado por ser dados grandes demais em comparação aos outros)\n",
    "\n",
    "    # --- Features Defasadas (Lags) ---\n",
    "    # Fornecem ao modelo uma \"memória\" de curto prazo sobre os indicadores-chave.\n",
    "    'lag_close_pct_1',\n",
    "    'lag_rsi_14_1',\n",
    "    'lag_macd_1',\n",
    "]\n",
    "\n",
    "    TARGET_COLUMN = 'close'\n",
    "\n",
    "    # O alvo (y) será binário: 1 se o preço subir, 0 se não\n",
    "    # df[TARGET_COLUMN].shift(-1) pega o preço de fechamento do *próximo* período\n",
    "    y = (df[TARGET_COLUMN].shift(-1) > df[TARGET_COLUMN]).astype(int)\n",
    "\n",
    "    # As features (X) serão os dados da FEATURE_COLUMNS\n",
    "    X_raw = df[FEATURE_COLUMNS]\n",
    "\n",
    "    num_features = len(FEATURE_COLUMNS)\n",
    "\n",
    "    print(f\"Quantidade de features definidas: {num_features}\")\n",
    "    print(f\"Shape dos dados brutos: {X_raw.shape}\")\n",
    "    print(f\"Valores mínimos {X_raw.min()}\")\n",
    "    print(f\"Valores máximos {X_raw.max()}\")\n",
    "    \n",
    "    return X_raw, y, FEATURE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_dados(X_raw):\n",
    "    \"\"\"Normaliza os dados de features.\"\"\"\n",
    "    # --- Normalização dos Dados ---\n",
    "\n",
    "    # Pipeline de normalização: Log transformation + Tanh transformation + Vector normalization\n",
    "    \n",
    "    # Função personalizada para transformação logaritma\n",
    "    def log_transform(X):\n",
    "        \"\"\"Aplica transformação logaritma aos dados\"\"\"\n",
    "        return np.log(X + 1)  # Adiciona 1 para evitar log(0)\n",
    "\n",
    "    # Função personalizada para transformação Tanh\n",
    "    def tanh_transform(X):\n",
    "        \"\"\"Aplica transformação tanh aos dados para reduzir outliers\"\"\"\n",
    "        return np.tanh(X)\n",
    "\n",
    "    # # Criar o pipeline de normalização\n",
    "    # scaler = Pipeline([\n",
    "    #     # ('log', FunctionTransformer(log_transform, validate=True)),\n",
    "    #     ('tanh', FunctionTransformer(tanh_transform, validate=True)),\n",
    "    #     ('normalize', Normalizer(norm='l2')),  # Normalização L2 (vetorial)\n",
    "    #     # ('minmax', MinMaxScaler(feature_range=(-1, 1)))\n",
    "    # ])\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "    print(f\"Dados normalizados. Shape: {X_scaled.shape}\")\n",
    "    print(f\"Min: {X_scaled.min():.6f}, Max: {X_scaled.max():.6f}\")\n",
    "    # print(f\"Norma L2 das primeiras 5 amostras: {np.linalg.norm(X_scaled[:5], axis=1)}\")\n",
    "    return X_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_sequencias(X_scaled, y, sequence_length):\n",
    "    \"\"\"Cria sequências de dados para o modelo.\"\"\"\n",
    "    X_sequences, y_sequences = [], []\n",
    "\n",
    "    # Verificar o tamanho correto para evitar index out of bounds\n",
    "    max_index = min(len(X_scaled), len(y)) - sequence_length\n",
    "\n",
    "    print(f\"Tamanho de X_scaled: {len(X_scaled)}\")\n",
    "    print(f\"Tamanho de y: {len(y)}\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "    print(f\"Max index para loop: {max_index}\")\n",
    "\n",
    "    for i in range(max_index):\n",
    "        X_sequences.append(X_scaled[i : i + sequence_length])\n",
    "        y_sequences.append(y[i + sequence_length - 1]) # O alvo corresponde ao final da sequência\n",
    "\n",
    "    X = np.array(X_sequences)\n",
    "    y = np.array(y_sequences)\n",
    "\n",
    "    print(f\"Sequências criadas. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_dados(X, y):\n",
    "    \"\"\"Divide os dados em conjuntos de treino e teste usando TimeSeriesSplit.\"\"\"\n",
    "    # --- Divisão em Treino e Teste ---\n",
    "    # Divisão dos dados em treino e teste\n",
    "    tscv = TimeSeriesSplit(n_splits=10)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Tentativa de balancear as classes, evitando a mesma reposta sempre\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    print(f\"Amostras de treino: {len(X_train)}\")\n",
    "    print(f\"Amostras de teste: {len(X_test)}\")\n",
    "    print(f\"Distribuição do target no treino: {np.bincount(y_train)}\")\n",
    "    print(f\"Distribuição do target no teste: {np.bincount(y_test)}\")\n",
    "    return X_train, X_test, y_train, y_test, class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_e_compilar_modelo(sequence_length, num_features):\n",
    "    \"\"\"\n",
    "    PASSO 3: Constrói e compila o modelo MLP.\n",
    "    Objetivo: Definir a arquitetura da nossa rede neural e compilá-la.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adiciona a camada de entrada com o formato correto\n",
    "    model.add(Input(shape=(sequence_length, num_features)))\n",
    "\n",
    "    # Camada de Entrada: Uso do Conv1D para capturar padrões temporais\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Camada de Pooling: Reduz a dimensionalidade e extrai características importantes\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Achata a saída do Conv1D para ser usada nas camadas densas\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    # 1ª Camada Oculta: Primeira camada que aprende os padrões\n",
    "    model.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # PReLU é uma ativação que pode aprender a inclinação\n",
    "    model.add(PReLU())\n",
    "\n",
    "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    # 2ª Camada Oculta: Segunda camada para aprender padrões mais complexos\n",
    "    model.add(Dense(units=32, kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # PReLU é uma ativação que pode aprender a inclinação\n",
    "    model.add(PReLU())\n",
    "\n",
    "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    # Camada de Saída: Gera a previsão final\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    # --- Compilação do Modelo ---\n",
    "    # O modelo é compilado com o otimizador Adam e a função de perda binary_crossentropy, adequada para problemas de classificação binária\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Mostra um resumo da arquitetura do modelo\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo(model, X_train, y_train, X_test, y_test, class_weight_dict):\n",
    "    \"\"\"\n",
    "    PASSO 4: Treina o modelo.\n",
    "    Objetivo: Alimentar o modelo com os dados de treino para que ele aprenda.\n",
    "    \"\"\"\n",
    "    # Callbacks para otimizar o treinamento\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000001)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=200,  # epochs: Quantas vezes o modelo verá todo o conjunto de dados de treino.\n",
    "        batch_size=16,  # batch_size: Quantas amostras o modelo vê antes de atualizar seus pesos.\n",
    "        validation_data=(X_test, y_test),  # Dados para validar o modelo a cada época.\n",
    "        class_weight=class_weight_dict,  # Pesos das classes para lidar com desbalanceamento\n",
    "        callbacks=callbacks,  # Callbacks para otimização do treinamento\n",
    "        verbose=1 # Mostra uma barra de progresso.\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelo(model, X_test, y_test, df_features_teste):\n",
    "    \"\"\"\n",
    "    PASSO 5: Avalia o modelo.\n",
    "    Objetivo: Verificar o quão bem o modelo se saiu nos dados de teste,\n",
    "              com a opção de aplicar um filtro condicional.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- DISTRIBUIÇÃO DE CLASSES NO CONJUNTO DE TESTE ---\")\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"Classe {label} ({'Sobe' if label == 1 else 'Não Sobe'}): {count} amostras ({count / len(y_test) * 100:.2f}%)\")\n",
    "\n",
    "    # Previsões globais (sem filtro)\n",
    "    print(\"\\n--- AVALIAÇÃO GLOBAL ---\")\n",
    "    probas_global = model.predict(X_test)\n",
    "    preds_global = (probas_global > 0.5).astype(int)\n",
    "\n",
    "    acc_global = accuracy_score(y_test, preds_global)\n",
    "    prec_global = precision_score(y_test, preds_global, zero_division=0)\n",
    "    recall_global = recall_score(y_test, preds_global, zero_division=0)\n",
    "    f1_global = f1_score(y_test, preds_global, zero_division=0)\n",
    "    auc_global = roc_auc_score(y_test, probas_global)\n",
    "\n",
    "    print(f\"Acurácia: {acc_global * 100:.2f}%\")\n",
    "    print(f\"Precisão: {prec_global * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall_global * 100:.2f}%\")\n",
    "    print(f\"F1-score: {f1_global * 100:.2f}%\")\n",
    "    print(f\"AUC/ROC: {auc_global:.4f}\")\n",
    "\n",
    "    print(\"\\nRelatório de Classificação (Global):\")\n",
    "    print(classification_report(y_test, preds_global, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
    "\n",
    "    # Matriz de confusão (global)\n",
    "    cm_global = confusion_matrix(y_test, preds_global)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_global, annot=True, fmt='d', cmap='Purples', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de Confusão (Global)')\n",
    "    plt.show()\n",
    "\n",
    "    condicao = otimizar_condicao(model, df_features_teste, y_test, min_amostras=10, metrica='f1_score')\n",
    "    if condicao is None:\n",
    "        print(\"Nenhuma condição de filtro foi otimizada. Avaliação condicional não será realizada.\")\n",
    "        return\n",
    "    \n",
    "    # Aplicar a condição otimizada\n",
    "    condicao = (df_features_teste['rsi_7'] > condicao['limite_superior']) | (df_features_teste['rsi_7'] < condicao['limite_inferior'])\n",
    "    filtro = condicao.iloc[-len(y_test):].values\n",
    "\n",
    "    print(f\"A condição será aplicada a {sum(filtro)} de {len(y_test)} amostras de teste ({sum(filtro)/len(y_test)*100:.2f}%).\")\n",
    "\n",
    "    if sum(filtro) == 0:\n",
    "        print(\"Nenhuma amostra de teste atendeu à condição especificada.\")\n",
    "        return\n",
    "\n",
    "    X_test_filtrado = X_test[filtro]\n",
    "    y_test_filtrado = y_test[filtro]\n",
    "\n",
    "    probas_filtrado = model.predict(X_test_filtrado)\n",
    "    preds_filtrado = (probas_filtrado > 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
    "    prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
    "    recall = recall_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
    "    f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
    "    auc = roc_auc_score(y_test_filtrado, probas_filtrado)\n",
    "\n",
    "    print(f\"Acurácia: {acc * 100:.2f}%\")\n",
    "    print(f\"Precisão: {prec * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1-score: {f1 * 100:.2f}%\")\n",
    "    print(f\"AUC/ROC: {auc:.4f}\")\n",
    "\n",
    "    print(\"\\nRelatório de Classificação (Filtrado):\")\n",
    "    print(classification_report(y_test_filtrado, preds_filtrado, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
    "\n",
    "    # Matriz de confusão (filtrada)\n",
    "    cm = confusion_matrix(y_test_filtrado, preds_filtrado)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de Confusão (Condicional)')\n",
    "    plt.show()\n",
    "\n",
    "def otimizar_condicao(self, df_features_teste, y_test, min_amostras=10, metrica='f1_score'):\n",
    "    \"\"\"\n",
    "    Passo 5.1: Otimiza a condição de filtro para avaliação do modelo.\n",
    "\n",
    "    Objetivo: Encontrar os melhores parâmetros para uma condição de filtro (ex: thresholds de RSI)\n",
    "              que maximize uma métrica de desempenho específica (ex: precisão ou F1-score)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- OTIMIZAÇÃO DE CONDIÇÃO (Métrica: {metrica}, Mín. Amostras: {min_amostras}) ---\")\n",
    "\n",
    "    # Espaço de busca para os parâmetros\n",
    "    limites_superiores_rsi = range(70, 96, 5)  # De 70 a 95, em passos de 5\n",
    "    limites_inferiores_rsi = range(5, 31, 5)   # De 5 a 30, em passos de 5\n",
    "\n",
    "    melhores_resultados = {\n",
    "        'pontuacao': -1.0,\n",
    "        'precisao': 0.0,\n",
    "        'f1_score': 0.0,\n",
    "        'acuracia': 0.0,\n",
    "        'num_amostras': 0,\n",
    "        'condicao': None,\n",
    "        'limite_superior': None,\n",
    "        'limite_inferior': None\n",
    "    }\n",
    "    \n",
    "    resultados_testados = []\n",
    "\n",
    "    # Iterar sobre todas as combinações de parâmetros\n",
    "    for sup in limites_superiores_rsi:\n",
    "        for inf in limites_inferiores_rsi:\n",
    "            # Cria a condição de filtro com os parâmetros atuais\n",
    "            condicao = (df_features_teste['rsi_7'] > sup) | (df_features_teste['rsi_7'] < inf)\n",
    "            \n",
    "            # Garante que o filtro tenha o mesmo tamanho que y_test\n",
    "            filtro = condicao.iloc[-len(y_test):].values\n",
    "            \n",
    "            num_amostras_filtradas = sum(filtro)\n",
    "\n",
    "            # Pula para a próxima iteração se o número de amostras for muito baixo\n",
    "            if num_amostras_filtradas < min_amostras:\n",
    "                continue\n",
    "\n",
    "            # Aplica o filtro aos dados de teste\n",
    "            X_test_filtrado = self.X_test[filtro]\n",
    "            y_test_filtrado = y_test[filtro]\n",
    "\n",
    "            # Faz previsões com o modelo nos dados filtrados\n",
    "            probas_filtrado = self.model.predict(X_test_filtrado)\n",
    "            preds_filtrado = (probas_filtrado > 0.5).astype(int)\n",
    "\n",
    "            # Calcula as métricas\n",
    "            acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
    "            prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
    "            f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
    "\n",
    "            # Armazena os resultados da iteração atual\n",
    "            resultados_testados.append({\n",
    "                'limite_superior': sup,\n",
    "                'limite_inferior': inf,\n",
    "                'precisao': prec,\n",
    "                'f1_score': f1,\n",
    "                'acuracia': acc,\n",
    "                'num_amostras': num_amostras_filtradas\n",
    "            })\n",
    "            \n",
    "            # Define a pontuação a ser otimizada\n",
    "            pontuacao_atual = 0.0\n",
    "            if metrica == 'precisao':\n",
    "                pontuacao_atual = prec\n",
    "            elif metrica == 'f1_score':\n",
    "                pontuacao_atual = f1\n",
    "            elif metrica == 'acuracia':\n",
    "                pontuacao_atual = acc\n",
    "\n",
    "            # Verifica se esta é a melhor pontuação encontrada até agora\n",
    "            if pontuacao_atual > melhores_resultados['pontuacao']:\n",
    "                melhores_resultados['pontuacao'] = pontuacao_atual\n",
    "                melhores_resultados['precisao'] = prec\n",
    "                melhores_resultados['f1_score'] = f1\n",
    "                melhores_resultados['acuracia'] = acc\n",
    "                melhores_resultados['num_amostras'] = num_amostras_filtradas\n",
    "                melhores_resultados['condicao'] = f\"RSI > {sup} ou RSI < {inf}\"\n",
    "                melhores_resultados['limite_superior'] = sup\n",
    "                melhores_resultados['limite_inferior'] = inf\n",
    "\n",
    "    if not resultados_testados:\n",
    "        print(\"Nenhuma combinação de parâmetros resultou em amostras suficientes para avaliação.\")\n",
    "        return None\n",
    "\n",
    "    # Exibe os melhores resultados encontrados\n",
    "    print(\"\\n--- MELHOR CONDIÇÃO ENCONTRADA ---\")\n",
    "    if melhores_resultados['condicao']:\n",
    "        print(f\"Condição: {melhores_resultados['condicao']}\")\n",
    "        print(f\"Número de Amostras: {melhores_resultados['num_amostras']} ({melhores_resultados['num_amostras'] / len(y_test) * 100:.2f}%)\")\n",
    "        print(f\"Métrica Otimizada ({metrica}): {melhores_resultados['pontuacao'] * 100:.2f}%\")\n",
    "        print(\"--- Métricas de Desempenho ---\")\n",
    "        print(f\"Acurácia: {melhores_resultados['acuracia'] * 100:.2f}%\")\n",
    "        print(f\"Precisão (Classe 'Sobe'): {melhores_resultados['precisao'] * 100:.2f}%\")\n",
    "        print(f\"F1-score (Classe 'Sobe'): {melhores_resultados['f1_score'] * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"Não foi possível encontrar uma condição ótima com os parâmetros fornecidos.\")\n",
    "        \n",
    "    # Exibir um ranking com as 5 melhores condições\n",
    "    df_resultados = pd.DataFrame(resultados_testados)\n",
    "    df_resultados = df_resultados.sort_values(by=metrica, ascending=False)\n",
    "    print(\"\\n--- Top 5 Melhores Condições ---\")\n",
    "    print(df_resultados.head())\n",
    "\n",
    "\n",
    "    # Retorna os parâmetros da melhor condição para uso posterior\n",
    "    return {\n",
    "        \"limite_superior\": melhores_resultados['limite_superior'],\n",
    "        \"limite_inferior\": melhores_resultados['limite_inferior']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_treinamento(history):\n",
    "    \"\"\"\n",
    "    PASSO 6: Visualiza o treinamento.\n",
    "    Objetivo: Plotar gráficos para ver se o modelo aprendeu bem ou se teve overfitting.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Gráfico da Acurácia\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
    "    plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
    "    plt.title('Acurácia ao Longo das Épocas')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gráfico da Perda (Loss)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Perda de Treino')\n",
    "    plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
    "    plt.title('Perda ao Longo das Épocas')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_modelo_e_scaler(model, scaler):\n",
    "    \"\"\"\n",
    "    PASSO 7: Salva o modelo e o normalizador.\n",
    "    Objetivo: Salvar o modelo treinado e o normalizador para uso futuro.\n",
    "    \"\"\"\n",
    "    # Por enquanto não é necessário salvar o modelo\n",
    "\n",
    "    MODEL_PATH = \"mlp_basic_model.h5\"\n",
    "    SCALER_PATH = \"basic_scaler.pkl\"\n",
    "\n",
    "    model.save(MODEL_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "\n",
    "    print(f\"Modelo salvo em: {MODEL_PATH}\")\n",
    "    print(f\"Normalizador salvo em: {SCALER_PATH}\")\n",
    "\n",
    "    print(\"Função 'salvar_modelo_e_scaler' chamada, mas a funcionalidade está comentada conforme o original.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8c7d6",
   "metadata": {},
   "source": [
    "## Execução Principal (Main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- Definições Globais ---\n",
    "    CAMINHO_ARQUIVO = r\"..\\..\\data\\fechamentos\\SOLUSDT_1h_data.csv\"\n",
    "    SEQUENCE_LENGTH = 36 # Quantos períodos olharemos para trás para prever o próximo\n",
    "\n",
    "    # --- PASSOS DO PROCESSO ---\n",
    "    # Passo 0: Puxar dados de outras criptos para comparação de comportamento\n",
    "    CAMINHO_ARQUIVO_BTC = r\"..\\..\\data\\fechamentos\\BTCUSDT_1h_data.csv\"\n",
    "    CAMINHO_ARQUIVO_ETH = r\"..\\..\\data\\fechamentos\\ETHUSDT_1h_data.csv\"\n",
    "\n",
    "    # PASSO 1: Carregar os dados\n",
    "    df_original = carregar_dados(CAMINHO_ARQUIVO)\n",
    "    df_comparacao_btc = carregar_dados(CAMINHO_ARQUIVO_BTC)\n",
    "    df_comparacao_eth = carregar_dados(CAMINHO_ARQUIVO_ETH)\n",
    "\n",
    "    # PASSO 2: Preparar features\n",
    "    df_features = preparar_features(df_original, df_comparacao_btc, df_comparacao_eth)\n",
    "\n",
    "    # PASSO 2.1: Definir features e alvo\n",
    "    X_raw, y_raw, feature_columns = definir_features_e_alvo(df_features)\n",
    "    NUM_FEATURES = len(feature_columns)\n",
    "\n",
    "    # PASSO 2.2: Normalizar dados\n",
    "    X_scaled, scaler = normalizar_dados(X_raw)\n",
    "    \n",
    "    # PASSO 2.3: Criar sequências\n",
    "    X_seq, y_seq = criar_sequencias(X_scaled, y_raw, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # PASSO 2.4: Dividir em treino e teste\n",
    "    X_train, X_test, y_train, y_test, class_weights = dividir_dados(X_seq, y_seq)\n",
    "    inicio_teste = len(df_features) - len(X_test)\n",
    "    df_features_teste = df_features.iloc[inicio_teste:].reset_index(drop=True)\n",
    "\n",
    "    # PASSO 3: Construir e compilar o modelo\n",
    "    modelo = construir_e_compilar_modelo(SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "    \n",
    "    # PASSO 4: Treinar o modelo\n",
    "    historico = treinar_modelo(modelo, X_train, y_train, X_test, y_test, class_weights)\n",
    "    \n",
    "    # PASSO 5: Avaliar o modelo\n",
    "    avaliar_modelo(modelo, X_test, y_test, df_features_teste)\n",
    "    \n",
    "    # PASSO 6: Visualizar o treinamento\n",
    "    visualizar_treinamento(historico)\n",
    "    \n",
    "    # PASSO 7: Salvar o modelo\n",
    "    # salvar_modelo_e_scaler(modelo, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2a1c0",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "### Já integrado:\n",
    "\n",
    "1. **Carrega dados históricos**\n",
    "2. **Prepara os dados**\n",
    "3. **Constrói um MLP**\n",
    "4. **Treina o modelo**\n",
    "5. **Avalia o desempenho**\n",
    "6. **Visualiza o treinamento**\n",
    "7. **Salva o modelo**\n",
    "\n",
    "### Próximos Passos:\n",
    "- ENTENDER POR QUE O MODELO SÓ FALA QUE VAI SUBIR (ERA POR CAUSA DO MINMAXSCALER, POR TER MUITOS OUTLIERS ELE APERTAVA VALORES INTERMEDIARIOS EM UM INTERVALO MUITO PEQUENO... TROQUEI PARA STANDARDSCALER)\n",
    "- Adicionar features técnicas, prepara-las e dar ao treino do modelo (não creio que esteja bom ainda)\n",
    "- Implementar validação cruzada temporal (implementei)\n",
    "- Testar diferentes janelas temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0e9b8",
   "metadata": {},
   "source": [
    "# Testes com outros modelos\n",
    "\n",
    "**Meio freestyle considerando que os dados são reciclados e não adaptados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com HistGradientBoosting ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# hgd_model = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "# hgd_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# hgd_predictions = hgd_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do HistGradientBoosting:\")\n",
    "# print(classification_report(y_test, hgd_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com RandomForest ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do RandomForest:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2a1c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com GradientBoosting ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# rf_model = GradientBoostingClassifier(\n",
    "#     n_estimators=200,         # Mais árvores pode melhorar, mas aumenta o tempo de treino\n",
    "#     learning_rate=0.05,       # Menor learning_rate pode melhorar generalização, mas exige mais árvores\n",
    "#     max_depth=4,              # Profundidade das árvores (3-5 é comum)\n",
    "#     subsample=0.8,            # Amostragem estocástica para robustez (0.8 é um bom começo)\n",
    "#     max_features='sqrt',      # Considera sqrt(n_features) em cada split (ajuda a evitar overfitting)\n",
    "#     random_state=42\n",
    "# )\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do GradientBoosting:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com StackingClassifier ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')),\n",
    "#     ('gb', GradientBoostingClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.05,\n",
    "#         max_depth=4,\n",
    "#         subsample=0.8,\n",
    "#         max_features='sqrt',\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ]\n",
    "\n",
    "# rf_model = StackingClassifier(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "#     n_jobs=-1,\n",
    "#     passthrough=False\n",
    "# )\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do StackingClassifier:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8c7d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com ExtraTreesClassifier ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# rf_model = ExtraTreesClassifier(\n",
    "#     n_estimators=200,         # Número de árvores na floresta\n",
    "#     max_depth=6,              # Profundidade máxima das árvores (ajuda a evitar overfitting)\n",
    "#     max_features='sqrt',      # Número de features consideradas em cada split\n",
    "#     min_samples_split=5,      # Mínimo de amostras para dividir um nó interno\n",
    "#     min_samples_leaf=3,       # Mínimo de amostras em cada folha\n",
    "#     class_weight='balanced',  # Balanceia as classes automaticamente\n",
    "#     n_jobs=-1,                # Usa todos os núcleos disponíveis\n",
    "#     random_state=42           # Reprodutibilidade\n",
    "# )\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do ExtraTreesClassifier:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
