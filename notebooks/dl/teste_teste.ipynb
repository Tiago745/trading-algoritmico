{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4f8d5a2",
      "metadata": {
        "id": "e4f8d5a2"
      },
      "source": [
        "# Modelo MLP para Previsão de Preços de Criptomoedas (Refatorado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e6b4d3",
      "metadata": {
        "id": "a9e6b4d3"
      },
      "source": [
        "## Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a0c2e1",
      "metadata": {
        "id": "f8a0c2e1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, Normalizer, FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, PReLU, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam, Adamax, AdamW, Lion, RMSprop\n",
        "import joblib\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c0a1f7",
      "metadata": {
        "id": "b9c0a1f7"
      },
      "source": [
        "## Funções do Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d3e5a6",
      "metadata": {
        "id": "c1d3e5a6"
      },
      "outputs": [],
      "source": [
        "def carregar_dados(caminho_arquivo, ultimas_linhas=100000):\n",
        "    \"\"\"\n",
        "    PASSO 1: Carrega e prepara os dados.\n",
        "    Objetivo: Ler o arquivo CSV e garantir que ele esteja em ordem.\n",
        "    \"\"\"\n",
        "    # Carrega os dados do arquivo CSV\n",
        "    df = pd.read_csv(caminho_arquivo)\n",
        "\n",
        "    # Evita o PC de explodir com muitos dados\n",
        "    df = df.iloc[-ultimas_linhas:]\n",
        "\n",
        "    # Garante que a coluna 'timestamp' seja do tipo data e ordena os dados\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp')\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Remove colunas com nomes \"Unnamed\".\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    print(f\"Dados carregados: {df.shape[0]} registros.\")\n",
        "    print(df.head())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e9b3f1",
      "metadata": {
        "id": "d7e9b3f1"
      },
      "outputs": [],
      "source": [
        "def preparar_features(df, df_btc=None, df_eth=None):\n",
        "    \"\"\"\n",
        "    PASSO 2: Preparar os Dados para o Modelo.\n",
        "    Objetivo: Transformar os dados brutos em um formato que a rede neural entenda (features e target).\n",
        "    \"\"\"\n",
        "    # --- Calculo dos Indicadores ---\n",
        "    df = df.copy()\n",
        "\n",
        "    if df_btc is not None:\n",
        "        btc_features = pd.DataFrame(index=df_btc.index)\n",
        "        btc_features['btc_close_pct'] = np.log1p(df_btc['close'].pct_change())\n",
        "        btc_features['btc_volume_pct'] = np.log1p(df_btc['volume'].pct_change())\n",
        "        btc_features['btc_price_trend'] = df_btc['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "        df = df.join(btc_features)\n",
        "        df['btc_correlation'] = df['close'].rolling(10).corr(df_btc['close'])\n",
        "\n",
        "    if df_eth is not None:\n",
        "        eth_features = pd.DataFrame(index=df_eth.index)\n",
        "        eth_features['eth_close_pct'] = np.log1p(df_eth['close'].pct_change())\n",
        "        eth_features['eth_volume_pct'] = np.log1p(df_eth['volume'].pct_change())\n",
        "        eth_features['eth_price_trend'] = df_eth['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "        df = df.join(eth_features)\n",
        "        df['eth_correlation'] = df['close'].rolling(10).corr(df_eth['close'])\n",
        "\n",
        "    # --- 1. Variações e transformações básicas ---\n",
        "    df['close_pct'] = np.log1p(df['close'].pct_change())\n",
        "    df['volume_pct'] = np.log1p(df['volume'].pct_change())\n",
        "\n",
        "    # --- 2. Médias móveis e suas diferenças ---\n",
        "    df['sma_10'] = df['close'].rolling(10).mean()\n",
        "    df['sma_30'] = df['close'].rolling(30).mean()\n",
        "    df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
        "    df['ema_30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
        "    df['sma_10_vs_sma_30'] = df['sma_10'] - df['sma_30']\n",
        "    df['price_vs_sma30'] = df['close'] - df['sma_30']\n",
        "\n",
        "    # --- 3. Indicadores técnicos comuns ---\n",
        "    def calc_rsi(series, period):\n",
        "        delta = series.diff()\n",
        "        gain = delta.clip(lower=0).rolling(period).mean()\n",
        "        loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
        "        rs = gain / (loss + 1e-10)\n",
        "        return 100 - 100 / (1 + rs)\n",
        "\n",
        "    for p in [7, 14, 21]:\n",
        "        df[f'rsi_{p}'] = calc_rsi(df['close'], p)\n",
        "\n",
        "    def macd(series, short=12, long=26, signal=9):\n",
        "        exp1 = series.ewm(span=short, adjust=False).mean()\n",
        "        exp2 = series.ewm(span=long, adjust=False).mean()\n",
        "        macd_line = exp1 - exp2\n",
        "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
        "        return macd_line, signal_line\n",
        "\n",
        "    df['macd'], df['macd_signal'] = macd(df['close'])\n",
        "    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
        "\n",
        "    def bollinger(series, window=20, std_dev=2):\n",
        "        ma = series.rolling(window).mean()\n",
        "        std = series.rolling(window).std()\n",
        "        upper = ma + std_dev * std\n",
        "        lower = ma - std_dev * std\n",
        "        return upper, lower\n",
        "\n",
        "    df['bb_upper'], df['bb_lower'] = bollinger(df['close'])\n",
        "    df['bb_upper_diff'] = df['bb_upper'] - df['close']\n",
        "    df['bb_lower_diff'] = df['close'] - df['bb_lower']\n",
        "\n",
        "    def atr(df, window=14):\n",
        "        tr = pd.concat([\n",
        "            df['high'] - df['low'],\n",
        "            abs(df['high'] - df['close'].shift()),\n",
        "            abs(df['low'] - df['close'].shift())\n",
        "        ], axis=1).max(axis=1)\n",
        "        return tr.rolling(window).mean()\n",
        "\n",
        "    df['atr_14'] = atr(df)\n",
        "\n",
        "    # --- 4. Osciladores ---\n",
        "    def stochastic(df, k_window=14, d_window=3):\n",
        "        low_min = df['low'].rolling(k_window).min()\n",
        "        high_max = df['high'].rolling(k_window).max()\n",
        "        k = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
        "        d = k.rolling(d_window).mean()\n",
        "        return k, d\n",
        "\n",
        "    df['so_k'], df['so_d'] = stochastic(df)\n",
        "\n",
        "    def williams_r(df, window=14):\n",
        "        low_min = df['low'].rolling(window).min()\n",
        "        high_max = df['high'].rolling(window).max()\n",
        "        return -100 * (high_max - df['close']) / (high_max - low_min + 1e-10)\n",
        "\n",
        "    df['wpr_14'] = williams_r(df)\n",
        "\n",
        "    # --- 5. Indicadores de volume ---\n",
        "    # OTIMIZAÇÃO: Lógica do OBV vetorizada para muito mais performance\n",
        "    price_direction = np.sign(df['close'].diff()).fillna(0)\n",
        "    df['obv'] = (price_direction * df['volume']).cumsum()\n",
        "\n",
        "    df['volume_trend'] = df['volume'].pct_change().rolling(10).mean()\n",
        "    df['price_trend'] = df['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "    # --- 6. Volatilidade e desvio padrão ---\n",
        "    for w in [10, 20, 40]:\n",
        "        df[f'std_{w}'] = df['close'].rolling(w).std()\n",
        "        df[f'volatility_{w}'] = df['close'].pct_change().rolling(w).std()\n",
        "\n",
        "    # --- 7. Lags de indicadores principais ---\n",
        "    cols_to_lag = [\n",
        "        'close_pct', 'rsi_7', 'rsi_14', 'rsi_21', 'macd', 'macd_signal',\n",
        "        'btc_close_pct', 'btc_volume_pct', 'btc_price_trend', 'btc_correlation',\n",
        "        'eth_close_pct', 'eth_volume_pct', 'eth_price_trend', 'eth_correlation'\n",
        "    ]\n",
        "\n",
        "    for col in cols_to_lag:\n",
        "        if col in df.columns:  # Apenas cria o lag se a coluna existir no df final\n",
        "            df[f'lag_{col}_1'] = df[col].shift(1)\n",
        "            df[f'lag_{col}_2'] = df[col].shift(2)\n",
        "            df[f'lag_{col}_3'] = df[col].shift(3)\n",
        "            df[f'lag_{col}_4'] = df[col].shift(4)\n",
        "            df[f'lag_{col}_5'] = df[col].shift(5)\n",
        "\n",
        "    cols_to_diff = [\n",
        "        'close', 'sma_10', 'sma_30', 'ema_10', 'ema_30',\n",
        "        'bb_upper', 'bb_lower', 'macd', 'macd_signal', 'obv',\n",
        "        'btc_close_pct', 'btc_volume_pct', 'btc_price_trend', 'rsi_7',\n",
        "        'rsi_14', 'rsi_21', 'eth_close_pct', 'eth_volume_pct', 'eth_price_trend'\n",
        "    ]\n",
        "\n",
        "    for col in cols_to_diff:\n",
        "        if col in df.columns:\n",
        "            df[f'diff_{col}_1'] = df[col].diff(1)\n",
        "            df[f'diff_{col}_2'] = df[col].diff(2)\n",
        "            df[f'diff_{col}_3'] = df[col].diff(3)\n",
        "            df[f'diff_{col}_4'] = df[col].diff(4)\n",
        "            df[f'diff_{col}_5'] = df[col].diff(5)\n",
        "\n",
        "    # --- 8. Finalização ---\n",
        "    df.dropna(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b2a8d1",
      "metadata": {
        "id": "e9b2a8d1"
      },
      "outputs": [],
      "source": [
        "def definir_features_e_alvo(df):\n",
        "    \"\"\"Define as colunas de features e a coluna alvo, e retorna os dados brutos X e y.\"\"\"\n",
        "    FEATURE_COLUMNS = ['btc_correlation', 'eth_price_trend', 'eth_correlation', 'volume_pct', 'sma_10', \n",
        "                       'rsi_14', 'wpr_14', 'obv', 'volume_trend', 'std_40', 'volatility_40', 'lag_close_pct_3', \n",
        "                       'lag_close_pct_4', 'lag_close_pct_5', 'lag_btc_close_pct_1', 'lag_btc_volume_pct_1', \n",
        "                       'lag_btc_volume_pct_3', 'lag_btc_volume_pct_4', 'lag_btc_volume_pct_5', 'lag_btc_correlation_5', \n",
        "                       'lag_eth_close_pct_1', 'lag_eth_volume_pct_1', 'lag_eth_volume_pct_2', 'lag_eth_volume_pct_3', \n",
        "                       'lag_eth_volume_pct_4', 'lag_eth_volume_pct_5', 'lag_eth_correlation_5', 'diff_bb_upper_1', \n",
        "                       'diff_obv_5', 'diff_btc_price_trend_1', 'diff_rsi_7_1', 'diff_rsi_7_2', 'diff_rsi_7_3', \n",
        "                       'diff_rsi_14_1', 'diff_rsi_14_3', 'diff_rsi_21_1', 'diff_rsi_21_3', 'diff_rsi_21_5', \n",
        "                       'diff_eth_volume_pct_3', 'diff_eth_price_trend_3']\n",
        "    TARGET_COLUMN = 'close'\n",
        "\n",
        "    # O alvo (y) será binário: 1 se o preço subir, 0 se não\n",
        "    # df[TARGET_COLUMN].shift(-1) pega o preço de fechamento do *próximo* período\n",
        "    y = (df[TARGET_COLUMN].shift(-1) > df[TARGET_COLUMN]).astype(int)\n",
        "\n",
        "    # As features (X) serão os dados da FEATURE_COLUMNS\n",
        "    X_raw = df[FEATURE_COLUMNS]\n",
        "\n",
        "    num_features = len(FEATURE_COLUMNS)\n",
        "\n",
        "    print(f\"Quantidade de features definidas: {num_features}\")\n",
        "    print(f\"Shape dos dados brutos: {X_raw.shape}\")\n",
        "    print(f\"Valores mínimos {X_raw.min()}\")\n",
        "    print(f\"Valores máximos {X_raw.max()}\")\n",
        "\n",
        "    return X_raw, y, FEATURE_COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c1a0e8",
      "metadata": {
        "id": "f5c1a0e8"
      },
      "outputs": [],
      "source": [
        "def normalizar_dados(X_raw):\n",
        "    \"\"\"Normaliza os dados de features.\"\"\"\n",
        "    # --- Normalização dos Dados ---\n",
        "\n",
        "    # Pipeline de normalização: Log transformation + Tanh transformation + Vector normalization\n",
        "\n",
        "    # Função personalizada para transformação logaritma\n",
        "    def log_transform(X):\n",
        "        \"\"\"Aplica transformação logaritma aos dados\"\"\"\n",
        "        return np.log(X + 1)  # Adiciona 1 para evitar log(0)\n",
        "\n",
        "    # Função personalizada para transformação Tanh\n",
        "    def tanh_transform(X):\n",
        "        \"\"\"Aplica transformação tanh aos dados para reduzir outliers\"\"\"\n",
        "        return np.tanh(X)\n",
        "\n",
        "    # # Criar o pipeline de normalização\n",
        "    # scaler = Pipeline([\n",
        "    #     # ('log', FunctionTransformer(log_transform, validate=True)),\n",
        "    #     ('tanh', FunctionTransformer(tanh_transform, validate=True)),\n",
        "    #     ('normalize', Normalizer(norm='l2')),  # Normalização L2 (vetorial)\n",
        "    #     # ('minmax', MinMaxScaler(feature_range=(-1, 1)))\n",
        "    # ])\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    X_scaled = scaler.fit_transform(X_raw)\n",
        "\n",
        "    print(f\"Dados normalizados. Shape: {X_scaled.shape}\")\n",
        "    print(f\"Min: {X_scaled.min():.6f}, Max: {X_scaled.max():.6f}\")\n",
        "    # print(f\"Norma L2 das primeiras 5 amostras: {np.linalg.norm(X_scaled[:5], axis=1)}\")\n",
        "    return X_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4",
      "metadata": {
        "id": "a1b2c3d4"
      },
      "outputs": [],
      "source": [
        "def criar_sequencias(X_scaled, y, sequence_length):\n",
        "    \"\"\"Cria sequências de dados para o modelo.\"\"\"\n",
        "    X_sequences, y_sequences = [], []\n",
        "\n",
        "    # Verificar o tamanho correto para evitar index out of bounds\n",
        "    max_index = min(len(X_scaled), len(y)) - sequence_length\n",
        "\n",
        "    print(f\"Tamanho de X_scaled: {len(X_scaled)}\")\n",
        "    print(f\"Tamanho de y: {len(y)}\")\n",
        "    print(f\"Sequence length: {sequence_length}\")\n",
        "    print(f\"Max index para loop: {max_index}\")\n",
        "\n",
        "    for i in range(max_index):\n",
        "        X_sequences.append(X_scaled[i : i + sequence_length])\n",
        "        y_sequences.append(y[i + sequence_length - 1]) # O alvo corresponde ao final da sequência\n",
        "\n",
        "    X = np.array(X_sequences)\n",
        "    y = np.array(y_sequences)\n",
        "\n",
        "    print(f\"Sequências criadas. X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f6a7b8",
      "metadata": {
        "id": "e5f6a7b8"
      },
      "outputs": [],
      "source": [
        "def dividir_dados(X, y, test_data=None, class_weight_multiplier=1.0):\n",
        "    \"\"\"Divide os dados em conjuntos de treino e teste usando TimeSeriesSplit.\"\"\"\n",
        "    # --- Divisão em Treino e Teste ---\n",
        "    # Divisão dos dados em treino e teste\n",
        "    tscv = TimeSeriesSplit(n_splits=5, test_size=test_data)\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Tentativa de balancear as classes, evitando a mesma reposta sempre\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Aumenta o peso da classe minoritária\n",
        "    if class_weight_dict[0] < class_weight_dict[1]:\n",
        "        class_weight_dict[0] *= class_weight_multiplier\n",
        "    else:\n",
        "        class_weight_dict[1] *= class_weight_multiplier\n",
        "\n",
        "\n",
        "    print(f\"Amostras de treino: {len(X_train)}\")\n",
        "    print(f\"Amostras de teste: {len(X_test)}\")\n",
        "    print(f\"Distribuição do target no treino: {np.bincount(y_train)}\")\n",
        "    print(f\"Distribuição do target no teste: {np.bincount(y_test)}\")\n",
        "    return X_train, X_test, y_train, y_test, class_weight_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d8e7f6",
      "metadata": {
        "id": "c9d8e7f6"
      },
      "outputs": [],
      "source": [
        "def binary_focal_loss(gamma=2., alpha=.5):\n",
        "    \"\"\"\"Função de perda focal binária.\n",
        "    Objetivo: Aumentar o peso das classes minoritárias e reduzir o impacto das classes majoritárias.\n",
        "    \"\"\"\n",
        "    # Função que retorna outra função para calcular a perda focal binária\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Função de perda focal binária.\n",
        "        Objetivo: Aumentar o peso das classes minoritárias e reduzir o impacto das classes majoritárias.\n",
        "        \"\"\"\n",
        "        # Se o rótulo for 1, pega a predição, senão usa 1 (neutro)\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        # Se o rótulo for 0, pega a predição, senão usa 0 (neutro)\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        # Evita problemas com log(0) usando um valor muito pequeno\n",
        "        epsilon = K.epsilon()\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Calcula o erro para a classe 1 com peso alpha e penalidade (1 - pt)^gamma\n",
        "        loss_1 = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))\n",
        "        # Calcula o erro para a classe 0 com peso (1 - alpha) e penalidade pt^gamma\n",
        "        loss_0 = -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "        # Soma total das perdas das duas classes\n",
        "        return loss_1 + loss_0\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "\n",
        "\n",
        "def construir_e_compilar_modelo(sequence_length, num_features, loss_function='binary_crossentropy'):\n",
        "    \"\"\"\n",
        "    PASSO 3: Constrói e compila o modelo MLP.\n",
        "    Objetivo: Definir a arquitetura da nossa rede neural e compilá-la.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adiciona a camada de entrada com o formato correto\n",
        "    model.add(Input(shape=(sequence_length, num_features)))\n",
        "\n",
        "    # Camada de Entrada: Uso do Conv1D para capturar padrões temporais\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Camada de Pooling: Reduz a dimensionalidade e extrai características importantes\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Achata a saída do Conv1D para ser usada nas camadas densas\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    # 1ª Camada Oculta: Primeira camada que aprende os padrões\n",
        "    model.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # PReLU é uma ativação que pode aprender a inclinação\n",
        "    model.add(PReLU())\n",
        "\n",
        "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
        "    model.add(Dropout(rate=0.1))\n",
        "\n",
        "    # 2ª Camada Oculta: Segunda camada para aprender padrões mais complexos\n",
        "    model.add(Dense(units=32, kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # PReLU é uma ativação que pode aprender a inclinação\n",
        "    model.add(PReLU())\n",
        "\n",
        "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
        "    model.add(Dropout(rate=0.1))\n",
        "\n",
        "    # Camada de Saída: Gera a previsão final\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    # --- Compilação do Modelo ---\n",
        "    # O modelo é compilado com o otimizador Adam e a função de perda binary_crossentropy, adequada para problemas de classificação binária\n",
        "    if loss_function == 'binary_focal_crossentropy':\n",
        "        loss = binary_focal_loss(gamma=2., alpha=.5)\n",
        "    else:\n",
        "        loss = loss_function\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Mostra um resumo da arquitetura do modelo\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a6d5c4",
      "metadata": {
        "id": "b7a6d5c4"
      },
      "outputs": [],
      "source": [
        "def treinar_modelo(model, X_train, y_train, X_test, y_test, class_weight_dict):\n",
        "    \"\"\"\n",
        "    PASSO 4: Treina o modelo.\n",
        "    Objetivo: Alimentar o modelo com os dados de treino para que ele aprenda.\n",
        "    \"\"\"\n",
        "    # Callbacks para otimizar o treinamento\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=200,  # epochs: Quantas vezes o modelo verá todo o conjunto de dados de treino.\n",
        "        batch_size=64,  # batch_size: Quantas amostras o modelo vê antes de atualizar seus pesos.\n",
        "        validation_data=(X_test, y_test),  # Dados para validar o modelo a cada época.\n",
        "        class_weight=class_weight_dict,  # Pesos das classes para lidar com desbalanceamento\n",
        "        callbacks=callbacks,  # Callbacks para otimização do treinamento\n",
        "        verbose=1 # Mostra uma barra de progresso.\n",
        "    )\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b4c3d2",
      "metadata": {
        "id": "a5b4c3d2"
      },
      "outputs": [],
      "source": [
        "def avaliar_modelo(model, X_test, y_test, df_features_teste, threshold=0.5):\n",
        "    \"\"\"\n",
        "    PASSO 5: Avalia o modelo.\n",
        "    Objetivo: Verificar o quão bem o modelo se saiu nos dados de teste,\n",
        "              com a opção de aplicar um filtro condicional.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- DISTRIBUIÇÃO DE CLASSES NO CONJUNTO DE TESTE ---\")\n",
        "    unique, counts = np.unique(y_test, return_counts=True)\n",
        "    for label, count in zip(unique, counts):\n",
        "        print(f\"Classe {label} ({'Sobe' if label == 1 else 'Não Sobe'}): {count} amostras ({count / len(y_test) * 100:.2f}%)\")\n",
        "\n",
        "    # Previsões globais (sem filtro)\n",
        "    print(\"\\n--- AVALIAÇÃO GLOBAL ---\")\n",
        "    probas_global = model.predict(X_test)\n",
        "    preds_global = (probas_global > threshold).astype(int)\n",
        "\n",
        "    acc_global = accuracy_score(y_test, preds_global)\n",
        "    prec_global = precision_score(y_test, preds_global, zero_division=0)\n",
        "    recall_global = recall_score(y_test, preds_global, zero_division=0)\n",
        "    f1_global = f1_score(y_test, preds_global, zero_division=0)\n",
        "    auc_global = roc_auc_score(y_test, probas_global)\n",
        "\n",
        "    print(f\"Acurácia: {acc_global * 100:.2f}%\")\n",
        "    print(f\"Precisão: {prec_global * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall_global * 100:.2f}%\")\n",
        "    print(f\"F1-score: {f1_global * 100:.2f}%\")\n",
        "    print(f\"AUC/ROC: {auc_global:.4f}\")\n",
        "\n",
        "    print(\"\\nRelatório de Classificação (Global):\")\n",
        "    print(classification_report(y_test, preds_global, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
        "\n",
        "    # Matriz de confusão (global)\n",
        "    cm_global = confusion_matrix(y_test, preds_global)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_global, annot=True, fmt='d', cmap='Purples', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusão (Global)')\n",
        "    plt.show()\n",
        "\n",
        "    # Otimização da condição\n",
        "    step_config = {\n",
        "        'rsi_7': 2,\n",
        "        'rsi_14': 2,\n",
        "        'so_d': 1,\n",
        "        'macd_diff': 0.02\n",
        "    }\n",
        "\n",
        "    params_to_optimize = {}\n",
        "    features_to_optimize = ['rsi_7', 'rsi_14', 'so_d', 'macd_diff']\n",
        "\n",
        "    for feature in features_to_optimize:\n",
        "        if feature not in df_features_teste.columns:\n",
        "            print(f\"Aviso: Feature '{feature}' não encontrada. Pulando otimização para esta feature.\")\n",
        "            continue\n",
        "\n",
        "        # Calcula os quartis e os limites (mínimo e máximo) da feature\n",
        "        q1 = df_features_teste[feature].quantile(0.25)\n",
        "        q3 = df_features_teste[feature].quantile(0.75)\n",
        "        min_val = df_features_teste[feature].min()\n",
        "        max_val = df_features_teste[feature].max()\n",
        "        step = step_config.get(feature, 1) # Usa o passo configurado ou 1 como padrão\n",
        "\n",
        "        # Define o range de busca para o limite superior (acima do 3º quartil)\n",
        "        # Adicionamos 'step' ao valor final porque np.arange é exclusivo\n",
        "        range_sup = [q3, max_val + step, step]\n",
        "\n",
        "        # Define o range de busca para o limite inferior (abaixo do 1º quartil)\n",
        "        range_inf = [min_val, q1 + step, step]\n",
        "\n",
        "        # Garante que os limites não se sobreponham e que o range seja válido\n",
        "        if range_sup[0] >= range_sup[1]: continue # Pula se Q3 for igual ou maior que o máximo\n",
        "        if range_inf[0] >= range_inf[1]: continue # Pula se o mínimo for igual ou maior que Q1\n",
        "\n",
        "        params_to_optimize[feature] = (range_sup, range_inf)\n",
        "\n",
        "        print(f\"\\nFeature: '{feature}'\")\n",
        "        print(f\"  - Intervalo de busca p/ Limite Superior (>= Q3): de {q3:.2f} a {max_val:.2f} (passo {step})\")\n",
        "        print(f\"  - Intervalo de busca p/ Limite Inferior (<= Q1): de {min_val:.2f} a {q1:.2f} (passo {step})\")\n",
        "\n",
        "    if not params_to_optimize:\n",
        "        print(\"\\nNão foi possível definir parâmetros de otimização. Avaliação condicional não será realizada.\")\n",
        "        return\n",
        "\n",
        "    # A chamada para otimizar_condicao usa os parâmetros dinâmicos\n",
        "    condicao_otima = otimizar_condicao(\n",
        "        model,\n",
        "        df_features_teste,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        params_to_optimize,\n",
        "        min_amostras=100,\n",
        "        metrica='f1_score'\n",
        "    )\n",
        "\n",
        "    if condicao_otima is None or not condicao_otima.get('condicao'):\n",
        "        print(\"\\nNenhuma condição de filtro foi otimizada. Avaliação condicional não será realizada.\")\n",
        "        return\n",
        "\n",
        "    feature_name = condicao_otima['feature']\n",
        "    limite_superior = condicao_otima['limite_superior']\n",
        "    limite_inferior = condicao_otima['limite_inferior']\n",
        "\n",
        "    condicao = (df_features_teste[feature_name] > limite_superior) | (df_features_teste[feature_name] < limite_inferior)\n",
        "    filtro = condicao.iloc[-len(y_test):].values\n",
        "\n",
        "    print(f\"\\nA condição otimizada '{condicao_otima['condicao']}' será aplicada a {sum(filtro)} de {len(y_test)} amostras de teste ({sum(filtro)/len(y_test)*100:.2f}%).\")\n",
        "\n",
        "    if sum(filtro) == 0:\n",
        "        print(\"Nenhuma amostra de teste atendeu à condição especificada.\")\n",
        "        return\n",
        "\n",
        "    X_test_filtrado = X_test[filtro]\n",
        "    y_test_filtrado = y_test[filtro]\n",
        "\n",
        "    probas_filtrado = model.predict(X_test_filtrado)\n",
        "    preds_filtrado = (probas_filtrado > threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
        "    prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    recall = recall_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    auc = roc_auc_score(y_test_filtrado, probas_filtrado)\n",
        "\n",
        "    print(f\"\\n--- AVALIAÇÃO COM FILTRO OTIMIZADO ({condicao_otima['condicao']}) ---\")\n",
        "    print(f\"Acurácia: {acc * 100:.2f}%\")\n",
        "    print(f\"Precisão: {prec * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall * 100:.2f}%\")\n",
        "    print(f\"F1-score: {f1 * 100:.2f}%\")\n",
        "    print(f\"AUC/ROC: {auc:.4f}\")\n",
        "\n",
        "    print(\"\\nRelatório de Classificação (Filtrado):\")\n",
        "    print(classification_report(y_test_filtrado, preds_filtrado, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
        "\n",
        "    cm = confusion_matrix(y_test_filtrado, preds_filtrado)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title(f\"Matriz de Confusão ({condicao_otima['condicao']})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def otimizar_condicao(model, df_features_teste, X_test, y_test, params_to_optimize, min_amostras=100, metrica='f1_score'):\n",
        "    \"\"\"\n",
        "    Passo 5.1: Otimiza a condição de filtro para avaliação do modelo. (VERSÃO OTIMIZADA)\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- OTIMIZAÇÃO DE CONDIÇÃO (Métrica: {metrica}, Mín. Amostras: {min_amostras}) ---\")\n",
        "\n",
        "    # Faça a predição para todo o conjunto de teste\n",
        "    print(\"Realizando predição única no conjunto de teste para otimização...\")\n",
        "    probas_test_full = model.predict(X_test, verbose=0).flatten()\n",
        "    preds_test_full = (probas_test_full > 0.5).astype(int)\n",
        "    print(\"Predição concluída. Iniciando busca por melhores condições...\")\n",
        "\n",
        "    melhores_resultados = {\n",
        "        'pontuacao': -1.0, 'precisao': 0.0, 'f1_score': 0.0, 'acuracia': 0.0,\n",
        "        'num_amostras': 0, 'condicao': None, 'feature': None,\n",
        "        'limite_superior': None, 'limite_inferior': None\n",
        "    }\n",
        "    resultados_testados = []\n",
        "\n",
        "    for feature, (range_sup, range_inf) in params_to_optimize.items():\n",
        "        start_sup, stop_sup, step_sup = range_sup\n",
        "        start_inf, stop_inf, step_inf = range_inf\n",
        "        limites_superiores = np.arange(start_sup, stop_sup, step_sup)\n",
        "        limites_inferiores = np.arange(start_inf, stop_inf, step_inf)\n",
        "\n",
        "        for sup in limites_superiores:\n",
        "            for inf in limites_inferiores:\n",
        "                condicao = (df_features_teste[feature] > sup) | (df_features_teste[feature] < inf)\n",
        "                filtro = condicao.iloc[-len(y_test):].values\n",
        "                num_amostras_filtradas = sum(filtro)\n",
        "\n",
        "                if num_amostras_filtradas < min_amostras:\n",
        "                    continue\n",
        "\n",
        "                # Em vez de prever de novo, apenas filtra os resultados já calculados\n",
        "                y_test_filtrado = y_test[filtro]\n",
        "                preds_filtrado = preds_test_full[filtro] # Filtra as predições\n",
        "\n",
        "                # Calcula as métricas com os dados já filtrados\n",
        "                acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
        "                prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
        "                f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
        "\n",
        "                resultados_testados.append({\n",
        "                    'feature': feature,\n",
        "                    'limite_superior': round(sup, 4),\n",
        "                    'limite_inferior': round(inf, 4),\n",
        "                    'precisao': prec,\n",
        "                    'f1_score': f1,\n",
        "                    'acuracia': acc,\n",
        "                    'num_amostras': num_amostras_filtradas\n",
        "                })\n",
        "\n",
        "                metric_map = {\n",
        "                    'acuracia': acc,\n",
        "                    'precisao': prec,\n",
        "                    'f1_score': f1\n",
        "                }\n",
        "                pontuacao_atual = metric_map.get(metrica, 0.0)\n",
        "\n",
        "                if pontuacao_atual > melhores_resultados['pontuacao']:\n",
        "                    melhores_resultados.update({\n",
        "                        'pontuacao': pontuacao_atual,\n",
        "                        'precisao': prec,\n",
        "                        'f1_score': f1,\n",
        "                        'acuracia': acc,\n",
        "                        'num_amostras': num_amostras_filtradas,\n",
        "                        'condicao': f\"{feature} > {sup:.2f} ou {feature} < {inf:.2f}\",\n",
        "                        'feature': feature,\n",
        "                        'limite_superior': sup,\n",
        "                        'limite_inferior': inf\n",
        "                    })\n",
        "\n",
        "    if not resultados_testados:\n",
        "        print(\"Nenhuma combinação de parâmetros resultou em amostras suficientes para avaliação.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n--- MELHOR CONDIÇÃO ENCONTRADA ---\")\n",
        "    if melhores_resultados['condicao']:\n",
        "        print(f\"Condição: {melhores_resultados['condicao']}\")\n",
        "        print(f\"Número de Amostras: {melhores_resultados['num_amostras']} ({melhores_resultados['num_amostras'] / len(y_test) * 100:.2f}%)\")\n",
        "        print(f\"Métrica Otimizada ({metrica}): {melhores_resultados['pontuacao'] * 100:.2f}%\")\n",
        "        print(f\"\\n--- Métricas de Desempenho ---\")\n",
        "        print(f\"Acurácia: {melhores_resultados['acuracia'] * 100:.2f}%\")\n",
        "        print(f\"Precisão (Classe 'Sobe'): {melhores_resultados['precisao'] * 100:.2f}%\")\n",
        "        print(f\"F1-score (Classe 'Sobe'): {melhores_resultados['f1_score'] * 100:.2f}%\")\n",
        "    else:\n",
        "        print(\"Não foi possível encontrar uma condição ótima com os parâmetros fornecidos.\")\n",
        "        return None\n",
        "\n",
        "    # Exibir os top 5 resultados em um DataFrame para análise\n",
        "    df_resultados = pd.DataFrame(resultados_testados)\n",
        "    df_resultados = df_resultados.sort_values(by=metrica, ascending=False)\n",
        "    print(\"\\n--- Top 5 Melhores Condições ---\")\n",
        "    print(df_resultados.head())\n",
        "\n",
        "    return melhores_resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f2a3b4",
      "metadata": {
        "id": "e1f2a3b4"
      },
      "outputs": [],
      "source": [
        "def visualizar_treinamento(history):\n",
        "    \"\"\"\n",
        "    PASSO 6: Visualiza o treinamento.\n",
        "    Objetivo: Plotar gráficos para ver se o modelo aprendeu bem ou se teve overfitting.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Gráfico da Acurácia\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
        "    plt.title('Acurácia ao Longo das Épocas')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "\n",
        "    # Gráfico da Perda (Loss)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Perda de Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
        "    plt.title('Perda ao Longo das Épocas')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d6e7f8",
      "metadata": {
        "id": "c5d6e7f8"
      },
      "outputs": [],
      "source": [
        "def salvar_modelo_e_scaler(model, scaler):\n",
        "    \"\"\"\n",
        "    PASSO 7: Salva o modelo e o normalizador.\n",
        "    Objetivo: Salvar o modelo treinado e o normalizador para uso futuro.\n",
        "    \"\"\"\n",
        "    # Por enquanto não é necessário salvar o modelo\n",
        "\n",
        "    MODEL_PATH = \"mlp_basic_model.h5\"\n",
        "    SCALER_PATH = \"basic_scaler.pkl\"\n",
        "\n",
        "    model.save(MODEL_PATH)\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "\n",
        "    print(f\"Modelo salvo em: {MODEL_PATH}\")\n",
        "    print(f\"Normalizador salvo em: {SCALER_PATH}\")\n",
        "\n",
        "    print(\"Função 'salvar_modelo_e_scaler' chamada, mas a funcionalidade está comentada conforme o original.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b8c7d6",
      "metadata": {
        "id": "a9b8c7d6"
      },
      "source": [
        "## Execução Principal (Main)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b249bf71",
      "metadata": {
        "id": "b249bf71"
      },
      "source": [
        "### Definições Globais e Carregamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44f45ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44f45ac",
        "outputId": "40c2db26-9a06-4a87-9393-ea12a3014892"
      },
      "outputs": [],
      "source": [
        "# --- Definições Globais ---\n",
        "CAMINHO_ARQUIVO = r\"../../data/fechamentos/SOLUSDT_15m_data.csv\"\n",
        "SEQUENCE_LENGTH = 36 # Quantos períodos olharemos para trás para prever o próximo\n",
        "\n",
        "# --- PASSOS DO PROCESSO ---\n",
        "# Passo 0: Puxar dados de outras criptos para comparação de comportamento\n",
        "CAMINHO_ARQUIVO_BTC = r\"../../data/fechamentos/BTCUSDT_15m_data.csv\"\n",
        "CAMINHO_ARQUIVO_ETH = r\"../../data/fechamentos/ETHUSDT_15m_data.csv\"\n",
        "\n",
        "# PASSO 1: Carregar os dados\n",
        "df_original = carregar_dados(CAMINHO_ARQUIVO)\n",
        "df_comparacao_btc = carregar_dados(CAMINHO_ARQUIVO_BTC)\n",
        "df_comparacao_eth = carregar_dados(CAMINHO_ARQUIVO_ETH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7bf2ed",
      "metadata": {
        "id": "1e7bf2ed"
      },
      "source": [
        "### Preparar Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd257ac2",
      "metadata": {
        "id": "fd257ac2"
      },
      "outputs": [],
      "source": [
        "# PASSO 2: Preparar features\n",
        "df_features = preparar_features(df_original, df_comparacao_btc, df_comparacao_eth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca205a2e",
      "metadata": {
        "id": "ca205a2e"
      },
      "source": [
        "### Definir Features e Alvo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2dddd42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2dddd42",
        "outputId": "098d1fdc-8204-4aed-9f64-f12e937060e5"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.1: Definir features e alvo\n",
        "X_raw, y_raw, feature_columns = definir_features_e_alvo(df_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f45718",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASSO 2.1.1: Selecionar features usando RFE (Recursive Feature Elimination)\n",
        "# Etapa opcional\n",
        "\n",
        "print(\"--- INICIANDO SELEÇÃO DE FEATURES COM WRAPPER METHOD (RFE) ---\")\n",
        "\n",
        "# O alvo (y_raw) tem um NaN na última linha. Vamos removê-lo para o treinamento do seletor\n",
        "X_for_selection = X_raw.iloc[:-1]\n",
        "y_for_selection = y_raw.iloc[:-1]\n",
        "\n",
        "# 1. Escolher o modelo \"proxy\" que será usado pelo RFE\n",
        "proxy_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# 2. Definir quantas das melhores features queremos selecionar no final\n",
        "N_FEATURES_TO_SELECT = 30\n",
        "\n",
        "# 3. Configurar o RFE (Recursive Feature Elimination)\n",
        "# - estimator: o modelo proxy que ele usará\n",
        "# - n_features_to_select: o número de features que devem restar no final\n",
        "# - step: quantas features remover a cada iteração. Um valor > 1 acelera o processo\n",
        "rfe = RFE(estimator=proxy_model, n_features_to_select=N_FEATURES_TO_SELECT, step=5, verbose=1)\n",
        "\n",
        "# 4. Executar o RFE. Esta é a parte demorada\n",
        "start_time = time.time()\n",
        "rfe.fit(X_for_selection, y_for_selection)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"\\nSeleção com RFE concluída em {end_time - start_time:.2f} segundos.\")\n",
        "\n",
        "# 5. Obter a lista das features selecionadas\n",
        "importances = rfe.estimator_.feature_importances_\n",
        "selected_features = X_for_selection.columns[rfe.support_]\n",
        "\n",
        "# Combinar em um DataFrame para visualização ordenada\n",
        "importancia_df = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Importância': importances\n",
        "}).sort_values(by='Importância', ascending=False)\n",
        "\n",
        "print(importancia_df)\n",
        "\n",
        "best_features = X_for_selection.columns[rfe.support_].tolist()\n",
        "\n",
        "print(f\"\\nAs {len(best_features)} features mais importantes selecionadas pelo RFE:\")\n",
        "print(best_features)\n",
        "\n",
        "# 6. Atualizar X_raw e feature_columns para usar apenas as melhores features\n",
        "X_raw = X_raw[best_features]\n",
        "feature_columns = best_features\n",
        "\n",
        "print(f\"\\nSeleção de features concluída. Novo shape de X_raw: {X_raw.shape}\")\n",
        "print(\"--- FIM DA SELEÇÃO DE FEATURES ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4536cb2",
      "metadata": {
        "id": "f4536cb2"
      },
      "source": [
        "### Normalizar Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607b5b42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607b5b42",
        "outputId": "826dc23e-1fb7-490f-975c-347a55c97dbc"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.2: Normalizar dados\n",
        "X_scaled, scaler = normalizar_dados(X_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1ca07f",
      "metadata": {
        "id": "2c1ca07f"
      },
      "source": [
        "### Criar Sequências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cdf3249",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cdf3249",
        "outputId": "3a032160-e679-49d5-f1a5-3c1dd416fb54"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.3: Criar sequências\n",
        "X_seq, y_seq = criar_sequencias(X_scaled, y_raw, SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab485074",
      "metadata": {
        "id": "ab485074"
      },
      "source": [
        "### Dividir em Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48252a11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48252a11",
        "outputId": "f525f9ed-a97e-468f-9c0e-7f4743375773"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.4: Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test, class_weights = dividir_dados(X_seq, y_seq)\n",
        "inicio_teste = len(df_features) - len(X_test)\n",
        "df_features_teste = df_features.iloc[inicio_teste:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7e2d23",
      "metadata": {
        "id": "5a7e2d23"
      },
      "source": [
        "### Construir e Compilar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c73dcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "65c73dcf",
        "outputId": "29dfb9c0-9359-42f4-952b-bd35ea944464"
      },
      "outputs": [],
      "source": [
        "# PASSO 3: Construir e compilar o modelo\n",
        "modelo = construir_e_compilar_modelo(SEQUENCE_LENGTH, len(feature_columns), loss_function='binary_focal_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7769e64b",
      "metadata": {
        "id": "7769e64b"
      },
      "source": [
        "### Treinar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e0f0dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25e0f0dc",
        "outputId": "6b4d61db-b126-4329-9653-144f928d4cde"
      },
      "outputs": [],
      "source": [
        "# PASSO 4: Treinar o modelo\n",
        "historico = treinar_modelo(modelo, X_train, y_train, X_test, y_test, class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18bf3d53",
      "metadata": {
        "id": "18bf3d53"
      },
      "source": [
        "### Avaliar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a163bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50a163bd",
        "outputId": "be9a9b68-2ab7-4878-9839-5cc50147b1b8"
      },
      "outputs": [],
      "source": [
        "# PASSO 5: Avaliar o modelo\n",
        "avaliar_modelo(modelo, X_test, y_test, df_features_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0ce984",
      "metadata": {
        "id": "ec0ce984"
      },
      "source": [
        "### Visualizar o Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4fb688",
      "metadata": {
        "id": "0b4fb688"
      },
      "outputs": [],
      "source": [
        "# PASSO 6: Visualizar o treinamento\n",
        "visualizar_treinamento(historico)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f054563",
      "metadata": {
        "id": "4f054563"
      },
      "source": [
        "### Salvar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3561a5",
      "metadata": {
        "id": "4a3561a5"
      },
      "outputs": [],
      "source": [
        "# PASSO 7: Salvar o modelo\n",
        "# salvar_modelo_e_scaler(modelo, scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f0f4ae",
      "metadata": {},
      "source": [
        "## H2O AutoML - Comparação com Modelo Neural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858c3bfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preparar_dados_h2o(df_features, feature_columns, target_column='close'):\n",
        "    \"\"\"\n",
        "    Prepara os dados para H2O AutoML usando as mesmas features do modelo neural.\n",
        "    \"\"\"    \n",
        "    # Criar uma cópia do DataFrame para não alterar o original\n",
        "    df_h2o = df_features.copy()\n",
        "    \n",
        "    # Criar a variável target binária (mesmo que no modelo neural)\n",
        "    df_h2o['target'] = (df_h2o[target_column].shift(-1) > df_h2o[target_column]).astype(int)\n",
        "    \n",
        "    # Remover a última linha que terá NaN no target\n",
        "    df_h2o = df_h2o.dropna()\n",
        "    \n",
        "    # Selecionar apenas as features escolhidas pelo RFE + target\n",
        "    columns_to_keep = feature_columns + ['target']\n",
        "    df_h2o = df_h2o[columns_to_keep]\n",
        "    \n",
        "    # Converter para H2OFrame\n",
        "    h2o_frame = h2o.H2OFrame(df_h2o)\n",
        "    \n",
        "    # Converter target para fator (categórica)\n",
        "    h2o_frame['target'] = h2o_frame['target'].asfactor()\n",
        "    \n",
        "    print(f\"Dados preparados para H2O: {h2o_frame.shape}\")\n",
        "    print(f\"Features utilizadas: {len(feature_columns)}\")\n",
        "    print(f\"Distribuição do target:\")\n",
        "    print(h2o_frame['target'].table())\n",
        "    \n",
        "    return h2o_frame\n",
        "\n",
        "\n",
        "def treinar_h2o_automl(h2o_frame, feature_columns, target='target', \n",
        "                       max_runtime_secs=300, max_models=30):\n",
        "    \"\"\"\n",
        "    Treina modelos usando H2O AutoML.\n",
        "    \"\"\"\n",
        "    # Dividir os dados seguindo a mesma lógica temporal do modelo neural\n",
        "    # Usar os últimos 20% para teste\n",
        "    split_ratio = 0.8\n",
        "    train, test = h2o_frame.split_frame(ratios=[split_ratio], seed=42)\n",
        "    \n",
        "    print(f\"Conjunto de treino H2O: {train.shape}\")\n",
        "    print(f\"Conjunto de teste H2O: {test.shape}\")\n",
        "    \n",
        "    # Configurar H2O AutoML\n",
        "    aml = H2OAutoML(\n",
        "        max_runtime_secs=max_runtime_secs,\n",
        "        max_models=max_models,\n",
        "        seed=42,\n",
        "        project_name=\"crypto_prediction_automl\",\n",
        "        sort_metric=\"AUC\",\n",
        "        balance_classes=True,\n",
        "        stopping_metric='AUC',\n",
        "        stopping_rounds=10,\n",
        "        stopping_tolerance=0.001,\n",
        "    )\n",
        "    \n",
        "    print(f\"Iniciando H2O AutoML (max_runtime: {max_runtime_secs}s, max_models: {max_models})...\")\n",
        "    \n",
        "    # Treinar os modelos\n",
        "    aml.train(x=feature_columns, y=target, training_frame=train, validation_frame=test)\n",
        "    \n",
        "    return aml, train, test\n",
        "\n",
        "def avaliar_h2o_automl(aml, test_frame):\n",
        "    \"\"\"\n",
        "    Avalia os resultados do H2O AutoML.\n",
        "    \"\"\"\n",
        "    # Mostrar leaderboard\n",
        "    print(\"\\n--- H2O AutoML LEADERBOARD ---\")\n",
        "    lb = aml.leaderboard\n",
        "    print(lb.head(rows=10))  # Top 10 modelos\n",
        "    \n",
        "    # Acessar o melhor modelo\n",
        "    leader_model = aml.leader\n",
        "    print(f\"\\n--- MELHOR MODELO H2O ---\")\n",
        "    print(f\"Algoritmo: {leader_model.__class__.__name__}\")\n",
        "    \n",
        "    # Fazer previsões\n",
        "    predictions = leader_model.predict(test_frame)\n",
        "    \n",
        "    # Avaliar performance\n",
        "    performance = leader_model.model_performance(test_frame)\n",
        "    \n",
        "    print(f\"\\n--- MÉTRICAS H2O AutoML ---\")\n",
        "    print(f\"AUC: {performance.auc():.4f}\")\n",
        "    print(f\"Acurácia: {performance.accuracy()[0][1]:.4f}\")\n",
        "    print(f\"Precisão: {performance.precision()[0][1]:.4f}\")\n",
        "    print(f\"Recall: {performance.recall()[0][1]:.4f}\")\n",
        "    print(f\"F1-Score: {performance.F1()[0][1]:.4f}\")\n",
        "    \n",
        "    # Matriz de confusão\n",
        "    print(f\"\\n--- MATRIZ DE CONFUSÃO H2O ---\")\n",
        "    cm = performance.confusion_matrix()\n",
        "    print(cm)\n",
        "    \n",
        "    return leader_model, predictions, performance\n",
        "\n",
        "def comparar_modelos(performance_h2o, historico_neural, y_test_neural, modelo_neural, X_test_neural):\n",
        "    \"\"\"\n",
        "    Compara os resultados entre H2O AutoML e o modelo neural.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPARAÇÃO: H2O AutoML vs Modelo Neural\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Métricas H2O\n",
        "    h2o_auc = performance_h2o.auc()\n",
        "    h2o_accuracy = performance_h2o.accuracy()[0][1]\n",
        "    h2o_precision = performance_h2o.precision()[0][1]\n",
        "    h2o_recall = performance_h2o.recall()[0][1]\n",
        "    h2o_f1 = performance_h2o.F1()[0][1]\n",
        "        \n",
        "    neural_probas = modelo_neural.predict(X_test_neural)\n",
        "    neural_preds = (neural_probas > 0.5).astype(int)\n",
        "    \n",
        "    neural_accuracy = accuracy_score(y_test_neural, neural_preds)\n",
        "    neural_precision = precision_score(y_test_neural, neural_preds, zero_division=0)\n",
        "    neural_recall = recall_score(y_test_neural, neural_preds, zero_division=0)\n",
        "    neural_f1 = f1_score(y_test_neural, neural_preds, zero_division=0)\n",
        "    neural_auc = roc_auc_score(y_test_neural, neural_probas)\n",
        "    \n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Métrica': ['AUC', 'Acurácia', 'Precisão', 'Recall', 'F1-Score'],\n",
        "        'H2O AutoML': [h2o_auc, h2o_accuracy, h2o_precision, h2o_recall, h2o_f1],\n",
        "        'Modelo Neural': [neural_auc, neural_accuracy, neural_precision, neural_recall, neural_f1]\n",
        "    })\n",
        "    \n",
        "    # Calcular diferenças\n",
        "    comparison_df['Diferença (H2O - Neural)'] = comparison_df['H2O AutoML'] - comparison_df['Modelo Neural']\n",
        "    comparison_df['Melhor'] = comparison_df['Diferença (H2O - Neural)'].apply(\n",
        "        lambda x: 'H2O AutoML' if x > 0 else 'Modelo Neural' if x < 0 else 'Empate'\n",
        "    )\n",
        "    \n",
        "    print(comparison_df.round(4))\n",
        "    \n",
        "    # Análise dos resultados\n",
        "    print(f\"\\n--- ANÁLISE COMPARATIVA ---\")\n",
        "    vencedor_count = comparison_df['Melhor'].value_counts()\n",
        "    print(f\"Vencedor por métrica:\")\n",
        "    for modelo, count in vencedor_count.items():\n",
        "        print(f\"  {modelo}: {count} métricas\")\n",
        "    \n",
        "    return comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189f526d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- EXECUÇÃO H2O AutoML INTEGRADO AO PIPELINE PRINCIPAL ---\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"INICIANDO H2O AutoML COM OS MESMOS DADOS DO MODELO NEURAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Inicializar cluster H2O\n",
        "print(\"Inicializando cluster H2O...\")\n",
        "h2o.init(max_mem_size=\"16G\")\n",
        "\n",
        "# 2. Preparar dados para H2O usando as mesmas features selecionadas pelo RFE\n",
        "print(\"Preparando dados para H2O AutoML...\")\n",
        "h2o_frame = preparar_dados_h2o(df_features, feature_columns, target_column='close')\n",
        "\n",
        "# 3. Treinar H2O AutoML\n",
        "print(\"Treinando modelos com H2O AutoML...\")\n",
        "aml_crypto, train_h2o, test_h2o = treinar_h2o_automl(\n",
        "    h2o_frame, \n",
        "    feature_columns, \n",
        "    target='target',\n",
        "    max_runtime_secs=600,\n",
        "    max_models=0\n",
        ")\n",
        "\n",
        "# 4. Avaliar resultados H2O\n",
        "print(\"Avaliando resultados H2O AutoML...\")\n",
        "leader_model_h2o, predictions_h2o, performance_h2o = avaliar_h2o_automl(aml_crypto, test_h2o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac3fa87",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Comparar H2O AutoML com Modelo Neural\n",
        "print(\"Comparando H2O AutoML com Modelo Neural...\")\n",
        "comparison_results = comparar_modelos(\n",
        "    performance_h2o, \n",
        "    historico, \n",
        "    y_test, \n",
        "    modelo, \n",
        "    X_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c06d454",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Análise detalhada dos melhores modelos H2O\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANÁLISE DETALHADA DOS MELHORES MODELOS H2O\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Mostrar os top 5 modelos com mais detalhes\n",
        "lb_detailed = aml_crypto.leaderboard\n",
        "print(\"Top 5 modelos H2O AutoML:\")\n",
        "for i in range(min(5, lb_detailed.nrows)):\n",
        "    model_id = lb_detailed[i, 'model_id']\n",
        "    auc = lb_detailed[i, 'auc']\n",
        "    mean_per_class_error = lb_detailed[i, 'mean_per_class_error']\n",
        "    print(f\"{i+1}. {model_id}: AUC={auc:.4f}, Mean Per Class Error={mean_per_class_error:.4f}\")\n",
        "\n",
        "# 7. Encerrar cluster H2O\n",
        "h2o.cluster().shutdown()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
