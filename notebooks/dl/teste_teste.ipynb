{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4f8d5a2",
      "metadata": {
        "id": "e4f8d5a2"
      },
      "source": [
        "# Modelo MLP para Previsão de Preços de Criptomoedas (Refatorado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e6b4d3",
      "metadata": {
        "id": "a9e6b4d3"
      },
      "source": [
        "## Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a0c2e1",
      "metadata": {
        "id": "f8a0c2e1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, Normalizer, FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, PReLU, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam, Adamax, AdamW, Lion, RMSprop\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c0a1f7",
      "metadata": {
        "id": "b9c0a1f7"
      },
      "source": [
        "## Funções do Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d3e5a6",
      "metadata": {
        "id": "c1d3e5a6"
      },
      "outputs": [],
      "source": [
        "def carregar_dados(caminho_arquivo, ultimas_linhas=100000):\n",
        "    \"\"\"\n",
        "    PASSO 1: Carrega e prepara os dados.\n",
        "    Objetivo: Ler o arquivo CSV e garantir que ele esteja em ordem.\n",
        "    \"\"\"\n",
        "    # Carrega os dados do arquivo CSV\n",
        "    df = pd.read_csv(caminho_arquivo)\n",
        "\n",
        "    # Evita o PC de explodir com muitos dados\n",
        "    df = df.iloc[-ultimas_linhas:]\n",
        "\n",
        "    # Garante que a coluna 'timestamp' seja do tipo data e ordena os dados\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp')\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Remove colunas com nomes \"Unnamed\".\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    print(f\"Dados carregados: {df.shape[0]} registros.\")\n",
        "    print(df.head())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e9b3f1",
      "metadata": {
        "id": "d7e9b3f1"
      },
      "outputs": [],
      "source": [
        "def preparar_features(df, df_btc=None, df_eth=None):\n",
        "    \"\"\"\n",
        "    PASSO 2: Preparar os Dados para o Modelo.\n",
        "    Objetivo: Transformar os dados brutos em um formato que a rede neural entenda (features e target).\n",
        "    \"\"\"\n",
        "    # --- Calculo dos Indicadores ---\n",
        "    df = df.copy()\n",
        "\n",
        "    if df_btc is not None:\n",
        "        btc_features = pd.DataFrame(index=df_btc.index)\n",
        "        btc_features['btc_close_pct'] = np.log1p(df_btc['close'].pct_change())\n",
        "        btc_features['btc_volume_pct'] = np.log1p(df_btc['volume'].pct_change())\n",
        "        btc_features['btc_price_trend'] = df_btc['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "        df = df.join(btc_features)\n",
        "        df['btc_correlation'] = df['close'].rolling(10).corr(df_btc['close'])\n",
        "\n",
        "    if df_eth is not None:\n",
        "        eth_features = pd.DataFrame(index=df_eth.index)\n",
        "        eth_features['eth_close_pct'] = np.log1p(df_eth['close'].pct_change())\n",
        "        eth_features['eth_volume_pct'] = np.log1p(df_eth['volume'].pct_change())\n",
        "        eth_features['eth_price_trend'] = df_eth['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "        df = df.join(eth_features)\n",
        "        df['eth_correlation'] = df['close'].rolling(10).corr(df_eth['close'])\n",
        "\n",
        "    # --- 1. Variações e transformações básicas ---\n",
        "    df['close_pct'] = np.log1p(df['close'].pct_change())\n",
        "    df['volume_pct'] = np.log1p(df['volume'].pct_change())\n",
        "\n",
        "    # --- 2. Médias móveis e suas diferenças ---\n",
        "    df['sma_10'] = df['close'].rolling(10).mean()\n",
        "    df['sma_30'] = df['close'].rolling(30).mean()\n",
        "    df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
        "    df['ema_30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
        "    df['sma_10_vs_sma_30'] = df['sma_10'] - df['sma_30']\n",
        "    df['price_vs_sma30'] = df['close'] - df['sma_30']\n",
        "\n",
        "    # --- 3. Indicadores técnicos comuns ---\n",
        "    def calc_rsi(series, period):\n",
        "        delta = series.diff()\n",
        "        gain = delta.clip(lower=0).rolling(period).mean()\n",
        "        loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
        "        rs = gain / (loss + 1e-10)\n",
        "        return 100 - 100 / (1 + rs)\n",
        "\n",
        "    for p in [7, 14, 21]:\n",
        "        df[f'rsi_{p}'] = calc_rsi(df['close'], p)\n",
        "\n",
        "    def macd(series, short=12, long=26, signal=9):\n",
        "        exp1 = series.ewm(span=short, adjust=False).mean()\n",
        "        exp2 = series.ewm(span=long, adjust=False).mean()\n",
        "        macd_line = exp1 - exp2\n",
        "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
        "        return macd_line, signal_line\n",
        "\n",
        "    df['macd'], df['macd_signal'] = macd(df['close'])\n",
        "    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
        "\n",
        "    def bollinger(series, window=20, std_dev=2):\n",
        "        ma = series.rolling(window).mean()\n",
        "        std = series.rolling(window).std()\n",
        "        upper = ma + std_dev * std\n",
        "        lower = ma - std_dev * std\n",
        "        return upper, lower\n",
        "\n",
        "    df['bb_upper'], df['bb_lower'] = bollinger(df['close'])\n",
        "    df['bb_upper_diff'] = df['bb_upper'] - df['close']\n",
        "    df['bb_lower_diff'] = df['close'] - df['bb_lower']\n",
        "\n",
        "    def atr(df, window=14):\n",
        "        tr = pd.concat([\n",
        "            df['high'] - df['low'],\n",
        "            abs(df['high'] - df['close'].shift()),\n",
        "            abs(df['low'] - df['close'].shift())\n",
        "        ], axis=1).max(axis=1)\n",
        "        return tr.rolling(window).mean()\n",
        "\n",
        "    df['atr_14'] = atr(df)\n",
        "\n",
        "    # --- 4. Osciladores ---\n",
        "    def stochastic(df, k_window=14, d_window=3):\n",
        "        low_min = df['low'].rolling(k_window).min()\n",
        "        high_max = df['high'].rolling(k_window).max()\n",
        "        k = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
        "        d = k.rolling(d_window).mean()\n",
        "        return k, d\n",
        "\n",
        "    df['so_k'], df['so_d'] = stochastic(df)\n",
        "\n",
        "    def williams_r(df, window=14):\n",
        "        low_min = df['low'].rolling(window).min()\n",
        "        high_max = df['high'].rolling(window).max()\n",
        "        return -100 * (high_max - df['close']) / (high_max - low_min + 1e-10)\n",
        "\n",
        "    df['wpr_14'] = williams_r(df)\n",
        "\n",
        "    # --- 5. Indicadores de volume ---\n",
        "    # OTIMIZAÇÃO: Lógica do OBV vetorizada para muito mais performance\n",
        "    price_direction = np.sign(df['close'].diff()).fillna(0)\n",
        "    df['obv'] = (price_direction * df['volume']).cumsum()\n",
        "\n",
        "    df['volume_trend'] = df['volume'].pct_change().rolling(10).mean()\n",
        "    df['price_trend'] = df['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "    # --- 6. Volatilidade e desvio padrão ---\n",
        "    for w in [10, 20, 40]:\n",
        "        df[f'std_{w}'] = df['close'].rolling(w).std()\n",
        "        df[f'volatility_{w}'] = df['close'].pct_change().rolling(w).std()\n",
        "\n",
        "    # --- 7. Lags de indicadores principais ---\n",
        "    cols_to_lag = [\n",
        "        'close_pct', 'rsi_7', 'rsi_14', 'rsi_21', 'macd', 'macd_signal',\n",
        "        'btc_close_pct', 'btc_volume_pct', 'btc_price_trend', 'btc_correlation',\n",
        "        'eth_close_pct', 'eth_volume_pct', 'eth_price_trend', 'eth_correlation'\n",
        "    ]\n",
        "\n",
        "    for col in cols_to_lag:\n",
        "        if col in df.columns:  # Apenas cria o lag se a coluna existir no df final\n",
        "            df[f'lag_{col}_1'] = df[col].shift(1)\n",
        "            df[f'lag_{col}_2'] = df[col].shift(2)\n",
        "\n",
        "    # --- 8. Finalização ---\n",
        "    df.dropna(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b2a8d1",
      "metadata": {
        "id": "e9b2a8d1"
      },
      "outputs": [],
      "source": [
        "def definir_features_e_alvo(df):\n",
        "    \"\"\"Define as colunas de features e a coluna alvo, e retorna os dados brutos X e y.\"\"\"\n",
        "    FEATURE_COLUMNS = [\n",
        "    # --- Features de Mercado Externo ---\n",
        "    # Essencial para capturar a tendência geral do mercado cripto.\n",
        "    'btc_close_pct',\n",
        "    'eth_close_pct',\n",
        "    'btc_price_trend',\n",
        "    'eth_price_trend',\n",
        "\n",
        "    # --- Features Principais do Ativo (DataFrame Principal) ---\n",
        "    # As variações de preço e volume são a base da predição.\n",
        "    'close_pct',\n",
        "    'volume_pct',\n",
        "    'price_trend',\n",
        "\n",
        "    # --- Indicadores de Tendência e Força ---\n",
        "    # Medem a direção e a força do movimento de preços.\n",
        "    'sma_10_vs_sma_30', # Cruzamento de médias, indica mudança de tendência\n",
        "    'price_vs_sma30',   # Distância do preço à média, indica sobre-extensão\n",
        "    'macd_diff',        # Histograma MACD, um dos sinais de momentum mais fortes\n",
        "\n",
        "    # --- Indicadores de Momento (Osciladores) ---\n",
        "    # Identificam condições de sobrecompra e sobrevenda.\n",
        "    'rsi_14',           # Índice de Força Relativa\n",
        "    'rsi_7',            # Índice de Força Relativa\n",
        "    'so_d',             # Linha de sinal do Estocástico, mais suave que %K\n",
        "\n",
        "    # --- Indicadores de Volatilidade ---\n",
        "    # Medem a magnitude das variações de preço.\n",
        "    'atr_14',           # Average True Range, mede a volatilidade real\n",
        "    'bb_upper_diff',    # Distância do preço à banda superior de Bollinger\n",
        "    'bb_lower_diff',    # Distância do preço à banda inferior de Bollinger\n",
        "\n",
        "    # --- Indicador de Volume ---\n",
        "    # Confirma a força da tendência com base no fluxo de volume.\n",
        "    # 'obv',              # On-Balance Volume (comentado por ser dados grandes demais em comparação aos outros)\n",
        "\n",
        "    # --- Features Defasadas (Lags) ---\n",
        "    # Fornecem ao modelo uma \"memória\" de curto prazo sobre os indicadores-chave.\n",
        "    'lag_close_pct_1',\n",
        "    'lag_rsi_14_1',\n",
        "    'lag_macd_1',\n",
        "]\n",
        "\n",
        "    TARGET_COLUMN = 'close'\n",
        "\n",
        "    # O alvo (y) será binário: 1 se o preço subir, 0 se não\n",
        "    # df[TARGET_COLUMN].shift(-1) pega o preço de fechamento do *próximo* período\n",
        "    y = (df[TARGET_COLUMN].shift(-1) > df[TARGET_COLUMN]).astype(int)\n",
        "\n",
        "    # As features (X) serão os dados da FEATURE_COLUMNS\n",
        "    X_raw = df[FEATURE_COLUMNS]\n",
        "\n",
        "    num_features = len(FEATURE_COLUMNS)\n",
        "\n",
        "    print(f\"Quantidade de features definidas: {num_features}\")\n",
        "    print(f\"Shape dos dados brutos: {X_raw.shape}\")\n",
        "    print(f\"Valores mínimos {X_raw.min()}\")\n",
        "    print(f\"Valores máximos {X_raw.max()}\")\n",
        "\n",
        "    return X_raw, y, FEATURE_COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c1a0e8",
      "metadata": {
        "id": "f5c1a0e8"
      },
      "outputs": [],
      "source": [
        "def normalizar_dados(X_raw):\n",
        "    \"\"\"Normaliza os dados de features.\"\"\"\n",
        "    # --- Normalização dos Dados ---\n",
        "\n",
        "    # Pipeline de normalização: Log transformation + Tanh transformation + Vector normalization\n",
        "\n",
        "    # Função personalizada para transformação logaritma\n",
        "    def log_transform(X):\n",
        "        \"\"\"Aplica transformação logaritma aos dados\"\"\"\n",
        "        return np.log(X + 1)  # Adiciona 1 para evitar log(0)\n",
        "\n",
        "    # Função personalizada para transformação Tanh\n",
        "    def tanh_transform(X):\n",
        "        \"\"\"Aplica transformação tanh aos dados para reduzir outliers\"\"\"\n",
        "        return np.tanh(X)\n",
        "\n",
        "    # # Criar o pipeline de normalização\n",
        "    # scaler = Pipeline([\n",
        "    #     # ('log', FunctionTransformer(log_transform, validate=True)),\n",
        "    #     ('tanh', FunctionTransformer(tanh_transform, validate=True)),\n",
        "    #     ('normalize', Normalizer(norm='l2')),  # Normalização L2 (vetorial)\n",
        "    #     # ('minmax', MinMaxScaler(feature_range=(-1, 1)))\n",
        "    # ])\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    X_scaled = scaler.fit_transform(X_raw)\n",
        "\n",
        "    print(f\"Dados normalizados. Shape: {X_scaled.shape}\")\n",
        "    print(f\"Min: {X_scaled.min():.6f}, Max: {X_scaled.max():.6f}\")\n",
        "    # print(f\"Norma L2 das primeiras 5 amostras: {np.linalg.norm(X_scaled[:5], axis=1)}\")\n",
        "    return X_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4",
      "metadata": {
        "id": "a1b2c3d4"
      },
      "outputs": [],
      "source": [
        "def criar_sequencias(X_scaled, y, sequence_length):\n",
        "    \"\"\"Cria sequências de dados para o modelo.\"\"\"\n",
        "    X_sequences, y_sequences = [], []\n",
        "\n",
        "    # Verificar o tamanho correto para evitar index out of bounds\n",
        "    max_index = min(len(X_scaled), len(y)) - sequence_length\n",
        "\n",
        "    print(f\"Tamanho de X_scaled: {len(X_scaled)}\")\n",
        "    print(f\"Tamanho de y: {len(y)}\")\n",
        "    print(f\"Sequence length: {sequence_length}\")\n",
        "    print(f\"Max index para loop: {max_index}\")\n",
        "\n",
        "    for i in range(max_index):\n",
        "        X_sequences.append(X_scaled[i : i + sequence_length])\n",
        "        y_sequences.append(y[i + sequence_length - 1]) # O alvo corresponde ao final da sequência\n",
        "\n",
        "    X = np.array(X_sequences)\n",
        "    y = np.array(y_sequences)\n",
        "\n",
        "    print(f\"Sequências criadas. X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f6a7b8",
      "metadata": {
        "id": "e5f6a7b8"
      },
      "outputs": [],
      "source": [
        "def dividir_dados(X, y, test_data=None, class_weight_multiplier=1.0):\n",
        "    \"\"\"Divide os dados em conjuntos de treino e teste usando TimeSeriesSplit.\"\"\"\n",
        "    # --- Divisão em Treino e Teste ---\n",
        "    # Divisão dos dados em treino e teste\n",
        "    tscv = TimeSeriesSplit(n_splits=5, test_size=test_data)\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Tentativa de balancear as classes, evitando a mesma reposta sempre\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Aumenta o peso da classe minoritária\n",
        "    if class_weight_dict[0] < class_weight_dict[1]:\n",
        "        class_weight_dict[0] *= class_weight_multiplier\n",
        "    else:\n",
        "        class_weight_dict[1] *= class_weight_multiplier\n",
        "\n",
        "\n",
        "    print(f\"Amostras de treino: {len(X_train)}\")\n",
        "    print(f\"Amostras de teste: {len(X_test)}\")\n",
        "    print(f\"Distribuição do target no treino: {np.bincount(y_train)}\")\n",
        "    print(f\"Distribuição do target no teste: {np.bincount(y_test)}\")\n",
        "    return X_train, X_test, y_train, y_test, class_weight_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d8e7f6",
      "metadata": {
        "id": "c9d8e7f6"
      },
      "outputs": [],
      "source": [
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\"Função de perda focal binária.\n",
        "    Objetivo: Aumentar o peso das classes minoritárias e reduzir o impacto das classes majoritárias.\n",
        "    \"\"\"\n",
        "    # Função que retorna outra função para calcular a perda focal binária\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Função de perda focal binária.\n",
        "        Objetivo: Aumentar o peso das classes minoritárias e reduzir o impacto das classes majoritárias.\n",
        "        \"\"\"\n",
        "        # Se o rótulo for 1, pega a predição, senão usa 1 (neutro)\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        # Se o rótulo for 0, pega a predição, senão usa 0 (neutro)\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        # Evita problemas com log(0) usando um valor muito pequeno\n",
        "        epsilon = K.epsilon()\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Calcula o erro para a classe 1 com peso alpha e penalidade (1 - pt)^gamma\n",
        "        loss_1 = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))\n",
        "        # Calcula o erro para a classe 0 com peso (1 - alpha) e penalidade pt^gamma\n",
        "        loss_0 = -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "        # Soma total das perdas das duas classes\n",
        "        return loss_1 + loss_0\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "\n",
        "\n",
        "def construir_e_compilar_modelo(sequence_length, num_features, loss_function='binary_crossentropy'):\n",
        "    \"\"\"\n",
        "    PASSO 3: Constrói e compila o modelo MLP.\n",
        "    Objetivo: Definir a arquitetura da nossa rede neural e compilá-la.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adiciona a camada de entrada com o formato correto\n",
        "    model.add(Input(shape=(sequence_length, num_features)))\n",
        "\n",
        "    # Camada de Entrada: Uso do Conv1D para capturar padrões temporais\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Camada de Pooling: Reduz a dimensionalidade e extrai características importantes\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Achata a saída do Conv1D para ser usada nas camadas densas\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    # 1ª Camada Oculta: Primeira camada que aprende os padrões\n",
        "    model.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.001)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # PReLU é uma ativação que pode aprender a inclinação\n",
        "    model.add(PReLU())\n",
        "\n",
        "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
        "    model.add(Dropout(rate=0.3))\n",
        "\n",
        "    # 2ª Camada Oculta: Segunda camada para aprender padrões mais complexos\n",
        "    model.add(Dense(units=32, kernel_regularizer=regularizers.l2(0.001)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # PReLU é uma ativação que pode aprender a inclinação\n",
        "    model.add(PReLU())\n",
        "\n",
        "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
        "    model.add(Dropout(rate=0.3))\n",
        "\n",
        "    # Camada de Saída: Gera a previsão final\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    # --- Compilação do Modelo ---\n",
        "    # O modelo é compilado com o otimizador Adam e a função de perda binary_crossentropy, adequada para problemas de classificação binária\n",
        "    if loss_function == 'binary_focal_crossentropy':\n",
        "        loss = binary_focal_loss(gamma=2., alpha=.25)\n",
        "    else:\n",
        "        loss = loss_function\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Mostra um resumo da arquitetura do modelo\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a6d5c4",
      "metadata": {
        "id": "b7a6d5c4"
      },
      "outputs": [],
      "source": [
        "def treinar_modelo(model, X_train, y_train, X_test, y_test, class_weight_dict):\n",
        "    \"\"\"\n",
        "    PASSO 4: Treina o modelo.\n",
        "    Objetivo: Alimentar o modelo com os dados de treino para que ele aprenda.\n",
        "    \"\"\"\n",
        "    # Callbacks para otimizar o treinamento\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000001)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=200,  # epochs: Quantas vezes o modelo verá todo o conjunto de dados de treino.\n",
        "        batch_size=16,  # batch_size: Quantas amostras o modelo vê antes de atualizar seus pesos.\n",
        "        validation_data=(X_test, y_test),  # Dados para validar o modelo a cada época.\n",
        "        class_weight=class_weight_dict,  # Pesos das classes para lidar com desbalanceamento\n",
        "        callbacks=callbacks,  # Callbacks para otimização do treinamento\n",
        "        verbose=1 # Mostra uma barra de progresso.\n",
        "    )\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b4c3d2",
      "metadata": {
        "id": "a5b4c3d2"
      },
      "outputs": [],
      "source": [
        "def avaliar_modelo(model, X_test, y_test, df_features_teste, threshold=0.5):\n",
        "    \"\"\"\n",
        "    PASSO 5: Avalia o modelo.\n",
        "    Objetivo: Verificar o quão bem o modelo se saiu nos dados de teste,\n",
        "              com a opção de aplicar um filtro condicional.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- DISTRIBUIÇÃO DE CLASSES NO CONJUNTO DE TESTE ---\")\n",
        "    unique, counts = np.unique(y_test, return_counts=True)\n",
        "    for label, count in zip(unique, counts):\n",
        "        print(f\"Classe {label} ({'Sobe' if label == 1 else 'Não Sobe'}): {count} amostras ({count / len(y_test) * 100:.2f}%)\")\n",
        "\n",
        "    # Previsões globais (sem filtro)\n",
        "    print(\"\\n--- AVALIAÇÃO GLOBAL ---\")\n",
        "    probas_global = model.predict(X_test)\n",
        "    preds_global = (probas_global > threshold).astype(int)\n",
        "\n",
        "    acc_global = accuracy_score(y_test, preds_global)\n",
        "    prec_global = precision_score(y_test, preds_global, zero_division=0)\n",
        "    recall_global = recall_score(y_test, preds_global, zero_division=0)\n",
        "    f1_global = f1_score(y_test, preds_global, zero_division=0)\n",
        "    auc_global = roc_auc_score(y_test, probas_global)\n",
        "\n",
        "    print(f\"Acurácia: {acc_global * 100:.2f}%\")\n",
        "    print(f\"Precisão: {prec_global * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall_global * 100:.2f}%\")\n",
        "    print(f\"F1-score: {f1_global * 100:.2f}%\")\n",
        "    print(f\"AUC/ROC: {auc_global:.4f}\")\n",
        "\n",
        "    print(\"\\nRelatório de Classificação (Global):\")\n",
        "    print(classification_report(y_test, preds_global, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
        "\n",
        "    # Matriz de confusão (global)\n",
        "    cm_global = confusion_matrix(y_test, preds_global)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_global, annot=True, fmt='d', cmap='Purples', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusão (Global)')\n",
        "    plt.show()\n",
        "\n",
        "    # Otimização da condição\n",
        "    step_config = {\n",
        "        'rsi_7': 2,\n",
        "        'rsi_14': 2,\n",
        "        'so_d': 1,\n",
        "        'macd_diff': 0.02\n",
        "    }\n",
        "\n",
        "    params_to_optimize = {}\n",
        "    features_to_optimize = ['rsi_7', 'rsi_14', 'so_d', 'macd_diff']\n",
        "\n",
        "    for feature in features_to_optimize:\n",
        "        if feature not in df_features_teste.columns:\n",
        "            print(f\"Aviso: Feature '{feature}' não encontrada. Pulando otimização para esta feature.\")\n",
        "            continue\n",
        "\n",
        "        # Calcula os quartis e os limites (mínimo e máximo) da feature\n",
        "        q1 = df_features_teste[feature].quantile(0.25)\n",
        "        q3 = df_features_teste[feature].quantile(0.75)\n",
        "        min_val = df_features_teste[feature].min()\n",
        "        max_val = df_features_teste[feature].max()\n",
        "        step = step_config.get(feature, 1) # Usa o passo configurado ou 1 como padrão\n",
        "\n",
        "        # Define o range de busca para o limite superior (acima do 3º quartil)\n",
        "        # Adicionamos 'step' ao valor final porque np.arange é exclusivo\n",
        "        range_sup = [q3, max_val + step, step]\n",
        "\n",
        "        # Define o range de busca para o limite inferior (abaixo do 1º quartil)\n",
        "        range_inf = [min_val, q1 + step, step]\n",
        "\n",
        "        # Garante que os limites não se sobreponham e que o range seja válido\n",
        "        if range_sup[0] >= range_sup[1]: continue # Pula se Q3 for igual ou maior que o máximo\n",
        "        if range_inf[0] >= range_inf[1]: continue # Pula se o mínimo for igual ou maior que Q1\n",
        "\n",
        "        params_to_optimize[feature] = (range_sup, range_inf)\n",
        "\n",
        "        print(f\"\\nFeature: '{feature}'\")\n",
        "        print(f\"  - Intervalo de busca p/ Limite Superior (>= Q3): de {q3:.2f} a {max_val:.2f} (passo {step})\")\n",
        "        print(f\"  - Intervalo de busca p/ Limite Inferior (<= Q1): de {min_val:.2f} a {q1:.2f} (passo {step})\")\n",
        "\n",
        "    if not params_to_optimize:\n",
        "        print(\"\\nNão foi possível definir parâmetros de otimização. Avaliação condicional não será realizada.\")\n",
        "        return\n",
        "\n",
        "    # A chamada para otimizar_condicao usa os parâmetros dinâmicos\n",
        "    condicao_otima = otimizar_condicao(\n",
        "        model,\n",
        "        df_features_teste,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        params_to_optimize,\n",
        "        min_amostras=100,\n",
        "        metrica='f1_score'\n",
        "    )\n",
        "\n",
        "    if condicao_otima is None or not condicao_otima.get('condicao'):\n",
        "        print(\"\\nNenhuma condição de filtro foi otimizada. Avaliação condicional não será realizada.\")\n",
        "        return\n",
        "\n",
        "    feature_name = condicao_otima['feature']\n",
        "    limite_superior = condicao_otima['limite_superior']\n",
        "    limite_inferior = condicao_otima['limite_inferior']\n",
        "\n",
        "    condicao = (df_features_teste[feature_name] > limite_superior) | (df_features_teste[feature_name] < limite_inferior)\n",
        "    filtro = condicao.iloc[-len(y_test):].values\n",
        "\n",
        "    print(f\"\\nA condição otimizada '{condicao_otima['condicao']}' será aplicada a {sum(filtro)} de {len(y_test)} amostras de teste ({sum(filtro)/len(y_test)*100:.2f}%).\")\n",
        "\n",
        "    if sum(filtro) == 0:\n",
        "        print(\"Nenhuma amostra de teste atendeu à condição especificada.\")\n",
        "        return\n",
        "\n",
        "    X_test_filtrado = X_test[filtro]\n",
        "    y_test_filtrado = y_test[filtro]\n",
        "\n",
        "    probas_filtrado = model.predict(X_test_filtrado)\n",
        "    preds_filtrado = (probas_filtrado > threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
        "    prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    recall = recall_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    auc = roc_auc_score(y_test_filtrado, probas_filtrado)\n",
        "\n",
        "    print(f\"\\n--- AVALIAÇÃO COM FILTRO OTIMIZADO ({condicao_otima['condicao']}) ---\")\n",
        "    print(f\"Acurácia: {acc * 100:.2f}%\")\n",
        "    print(f\"Precisão: {prec * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall * 100:.2f}%\")\n",
        "    print(f\"F1-score: {f1 * 100:.2f}%\")\n",
        "    print(f\"AUC/ROC: {auc:.4f}\")\n",
        "\n",
        "    print(\"\\nRelatório de Classificação (Filtrado):\")\n",
        "    print(classification_report(y_test_filtrado, preds_filtrado, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
        "\n",
        "    cm = confusion_matrix(y_test_filtrado, preds_filtrado)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title(f\"Matriz de Confusão ({condicao_otima['condicao']})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def otimizar_condicao(model, df_features_teste, X_test, y_test, params_to_optimize, min_amostras=100, metrica='f1_score'):\n",
        "    \"\"\"\n",
        "    Passo 5.1: Otimiza a condição de filtro para avaliação do modelo.\n",
        "\n",
        "    Objetivo: Encontrar a melhor combinação de indicador e limites (thresholds)\n",
        "              que maximize uma métrica de desempenho específica.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- OTIMIZAÇÃO DE CONDIÇÃO (Métrica: {metrica}, Mín. Amostras: {min_amostras}) ---\")\n",
        "\n",
        "    melhores_resultados = {\n",
        "        'pontuacao': -1.0,\n",
        "        'precisao': 0.0,\n",
        "        'f1_score': 0.0,\n",
        "        'acuracia': 0.0,\n",
        "        'num_amostras': 0,\n",
        "        'condicao': None,\n",
        "        'feature': None,\n",
        "        'limite_superior': None,\n",
        "        'limite_inferior': None\n",
        "    }\n",
        "    resultados_testados = []\n",
        "\n",
        "    # Iterar sobre cada feature e seus respectivos ranges de busca\n",
        "    for feature, (range_sup, range_inf) in params_to_optimize.items():\n",
        "        # Desempacotar início, fim e passo para os limites superior e inferior\n",
        "        start_sup, stop_sup, step_sup = range_sup\n",
        "        start_inf, stop_inf, step_inf = range_inf\n",
        "\n",
        "        # Gerar os limites (thresholds) a serem testados usando np.arange\n",
        "        limites_superiores = np.arange(start_sup, stop_sup, step_sup)\n",
        "        limites_inferiores = np.arange(start_inf, stop_inf, step_inf)\n",
        "\n",
        "        # Iterar sobre todas as combinações de limites\n",
        "        for sup in limites_superiores:\n",
        "            for inf in limites_inferiores:\n",
        "                # Cria a condição de filtro com os parâmetros atuais\n",
        "                condicao = (df_features_teste[feature] > sup) | (df_features_teste[feature] < inf)\n",
        "                filtro = condicao.iloc[-len(y_test):].values\n",
        "                num_amostras_filtradas = sum(filtro)\n",
        "\n",
        "                if num_amostras_filtradas < min_amostras:\n",
        "                    continue\n",
        "\n",
        "                X_test_filtrado = X_test[filtro]\n",
        "                y_test_filtrado = y_test[filtro]\n",
        "\n",
        "                # Adicionado verbose=0 para não poluir a saída do console durante a otimização\n",
        "                probas_filtrado = model.predict(X_test_filtrado, verbose=0)\n",
        "                preds_filtrado = (probas_filtrado > 0.5).astype(int)\n",
        "\n",
        "                acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
        "                prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
        "                f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
        "\n",
        "                resultados_testados.append({\n",
        "                    'feature': feature,\n",
        "                    'limite_superior': round(sup, 4),\n",
        "                    'limite_inferior': round(inf, 4),\n",
        "                    'precisao': prec,\n",
        "                    'f1_score': f1,\n",
        "                    'acuracia': acc,\n",
        "                    'num_amostras': num_amostras_filtradas\n",
        "                })\n",
        "\n",
        "                metric_map = {\n",
        "                    'acuracia': acc,\n",
        "                    'precisao': prec,\n",
        "                    'f1_score': f1\n",
        "                }\n",
        "                pontuacao_atual = metric_map.get(metrica, 0.0)\n",
        "\n",
        "                if pontuacao_atual > melhores_resultados['pontuacao']:\n",
        "                    melhores_resultados.update({\n",
        "                        'pontuacao': pontuacao_atual,\n",
        "                        'precisao': prec,\n",
        "                        'f1_score': f1,\n",
        "                        'acuracia': acc,\n",
        "                        'num_amostras': num_amostras_filtradas,\n",
        "                        'condicao': f\"{feature} > {sup:.2f} ou {feature} < {inf:.2f}\",\n",
        "                        'feature': feature,\n",
        "                        'limite_superior': sup,\n",
        "                        'limite_inferior': inf\n",
        "                    })\n",
        "\n",
        "    if not resultados_testados:\n",
        "        print(\"Nenhuma combinação de parâmetros resultou em amostras suficientes para avaliação.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n--- MELHOR CONDIÇÃO ENCONTRADA ---\")\n",
        "    if melhores_resultados['condicao']:\n",
        "        print(f\"Condição: {melhores_resultados['condicao']}\")\n",
        "        print(f\"Número de Amostras: {melhores_resultados['num_amostras']} ({melhores_resultados['num_amostras'] / len(y_test) * 100:.2f}%)\")\n",
        "        print(f\"Métrica Otimizada ({metrica}): {melhores_resultados['pontuacao'] * 100:.2f}%\")\n",
        "        print(f\"\\n--- Métricas de Desempenho ---\")\n",
        "        print(f\"Acurácia: {melhores_resultados['acuracia'] * 100:.2f}%\")\n",
        "        print(f\"Precisão (Classe 'Sobe'): {melhores_resultados['precisao'] * 100:.2f}%\")\n",
        "        print(f\"F1-score (Classe 'Sobe'): {melhores_resultados['f1_score'] * 100:.2f}%\")\n",
        "    else:\n",
        "        print(\"Não foi possível encontrar uma condição ótima com os parâmetros fornecidos.\")\n",
        "        return None\n",
        "\n",
        "    # Exibir os top 5 resultados em um DataFrame para análise\n",
        "    df_resultados = pd.DataFrame(resultados_testados)\n",
        "    df_resultados = df_resultados.sort_values(by=metrica, ascending=False)\n",
        "    print(\"\\n--- Top 5 Melhores Condições ---\")\n",
        "    print(df_resultados.head())\n",
        "\n",
        "    return melhores_resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f2a3b4",
      "metadata": {
        "id": "e1f2a3b4"
      },
      "outputs": [],
      "source": [
        "def visualizar_treinamento(history):\n",
        "    \"\"\"\n",
        "    PASSO 6: Visualiza o treinamento.\n",
        "    Objetivo: Plotar gráficos para ver se o modelo aprendeu bem ou se teve overfitting.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Gráfico da Acurácia\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
        "    plt.title('Acurácia ao Longo das Épocas')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "\n",
        "    # Gráfico da Perda (Loss)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Perda de Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
        "    plt.title('Perda ao Longo das Épocas')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d6e7f8",
      "metadata": {
        "id": "c5d6e7f8"
      },
      "outputs": [],
      "source": [
        "def salvar_modelo_e_scaler(model, scaler):\n",
        "    \"\"\"\n",
        "    PASSO 7: Salva o modelo e o normalizador.\n",
        "    Objetivo: Salvar o modelo treinado e o normalizador para uso futuro.\n",
        "    \"\"\"\n",
        "    # Por enquanto não é necessário salvar o modelo\n",
        "\n",
        "    MODEL_PATH = \"mlp_basic_model.h5\"\n",
        "    SCALER_PATH = \"basic_scaler.pkl\"\n",
        "\n",
        "    model.save(MODEL_PATH)\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "\n",
        "    print(f\"Modelo salvo em: {MODEL_PATH}\")\n",
        "    print(f\"Normalizador salvo em: {SCALER_PATH}\")\n",
        "\n",
        "    print(\"Função 'salvar_modelo_e_scaler' chamada, mas a funcionalidade está comentada conforme o original.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b8c7d6",
      "metadata": {
        "id": "a9b8c7d6"
      },
      "source": [
        "## Execução Principal (Main)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b249bf71",
      "metadata": {
        "id": "b249bf71"
      },
      "source": [
        "### Definições Globais e Carregamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44f45ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44f45ac",
        "outputId": "40c2db26-9a06-4a87-9393-ea12a3014892"
      },
      "outputs": [],
      "source": [
        "# --- Definições Globais ---\n",
        "CAMINHO_ARQUIVO = r\"/content/drive/MyDrive/data/SOLUSDT_15m_data.csv\"\n",
        "SEQUENCE_LENGTH = 36 # Quantos períodos olharemos para trás para prever o próximo\n",
        "\n",
        "# --- PASSOS DO PROCESSO ---\n",
        "# Passo 0: Puxar dados de outras criptos para comparação de comportamento\n",
        "CAMINHO_ARQUIVO_BTC = r\"/content/drive/MyDrive/data/BTCUSDT_15m_data.csv\"\n",
        "CAMINHO_ARQUIVO_ETH = r\"/content/drive/MyDrive/data/ETHUSDT_15m_data.csv\"\n",
        "\n",
        "# PASSO 1: Carregar os dados\n",
        "df_original = carregar_dados(CAMINHO_ARQUIVO)\n",
        "df_comparacao_btc = carregar_dados(CAMINHO_ARQUIVO_BTC)\n",
        "df_comparacao_eth = carregar_dados(CAMINHO_ARQUIVO_ETH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7bf2ed",
      "metadata": {
        "id": "1e7bf2ed"
      },
      "source": [
        "### Preparar Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd257ac2",
      "metadata": {
        "id": "fd257ac2"
      },
      "outputs": [],
      "source": [
        "# PASSO 2: Preparar features\n",
        "df_features = preparar_features(df_original, df_comparacao_btc, df_comparacao_eth)\n",
        "df_features.to_csv('df_features.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca205a2e",
      "metadata": {
        "id": "ca205a2e"
      },
      "source": [
        "### Definir Features e Alvo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2dddd42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2dddd42",
        "outputId": "098d1fdc-8204-4aed-9f64-f12e937060e5"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.1: Definir features e alvo\n",
        "X_raw, y_raw, feature_columns = definir_features_e_alvo(df_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4536cb2",
      "metadata": {
        "id": "f4536cb2"
      },
      "source": [
        "### Normalizar Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607b5b42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607b5b42",
        "outputId": "826dc23e-1fb7-490f-975c-347a55c97dbc"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.2: Normalizar dados\n",
        "X_scaled, scaler = normalizar_dados(X_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1ca07f",
      "metadata": {
        "id": "2c1ca07f"
      },
      "source": [
        "### Criar Sequências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cdf3249",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cdf3249",
        "outputId": "3a032160-e679-49d5-f1a5-3c1dd416fb54"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.3: Criar sequências\n",
        "X_seq, y_seq = criar_sequencias(X_scaled, y_raw, SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab485074",
      "metadata": {
        "id": "ab485074"
      },
      "source": [
        "### Dividir em Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48252a11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48252a11",
        "outputId": "f525f9ed-a97e-468f-9c0e-7f4743375773"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.4: Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test, class_weights = dividir_dados(X_seq, y_seq)\n",
        "inicio_teste = len(df_features) - len(X_test)\n",
        "df_features_teste = df_features.iloc[inicio_teste:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7e2d23",
      "metadata": {
        "id": "5a7e2d23"
      },
      "source": [
        "### Construir e Compilar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c73dcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "65c73dcf",
        "outputId": "29dfb9c0-9359-42f4-952b-bd35ea944464"
      },
      "outputs": [],
      "source": [
        "# PASSO 3: Construir e compilar o modelo\n",
        "modelo = construir_e_compilar_modelo(SEQUENCE_LENGTH, len(feature_columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7769e64b",
      "metadata": {
        "id": "7769e64b"
      },
      "source": [
        "### Treinar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e0f0dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25e0f0dc",
        "outputId": "6b4d61db-b126-4329-9653-144f928d4cde"
      },
      "outputs": [],
      "source": [
        "# PASSO 4: Treinar o modelo\n",
        "historico = treinar_modelo(modelo, X_train, y_train, X_test, y_test, class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18bf3d53",
      "metadata": {
        "id": "18bf3d53"
      },
      "source": [
        "### Avaliar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a163bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50a163bd",
        "outputId": "be9a9b68-2ab7-4878-9839-5cc50147b1b8"
      },
      "outputs": [],
      "source": [
        "# PASSO 5: Avaliar o modelo\n",
        "avaliar_modelo(modelo, X_test, y_test, df_features_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0ce984",
      "metadata": {
        "id": "ec0ce984"
      },
      "source": [
        "### Visualizar o Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4fb688",
      "metadata": {
        "id": "0b4fb688"
      },
      "outputs": [],
      "source": [
        "# PASSO 6: Visualizar o treinamento\n",
        "visualizar_treinamento(historico)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f054563",
      "metadata": {
        "id": "4f054563"
      },
      "source": [
        "### Salvar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3561a5",
      "metadata": {
        "id": "4a3561a5"
      },
      "outputs": [],
      "source": [
        "# PASSO 7: Salvar o modelo\n",
        "# salvar_modelo_e_scaler(modelo, scaler)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
