{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4f8d5a2",
      "metadata": {
        "id": "e4f8d5a2"
      },
      "source": [
        "# Modelo MLP para Previsão de Preços de Criptomoedas (Refatorado)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e6b4d3",
      "metadata": {
        "id": "a9e6b4d3"
      },
      "source": [
        "## Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "f8a0c2e1",
      "metadata": {
        "id": "f8a0c2e1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, Normalizer, FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, PReLU, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam, Adamax, AdamW, Lion, RMSprop\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c0a1f7",
      "metadata": {
        "id": "b9c0a1f7"
      },
      "source": [
        "## Funções do Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "c1d3e5a6",
      "metadata": {
        "id": "c1d3e5a6"
      },
      "outputs": [],
      "source": [
        "def carregar_dados(caminho_arquivo, ultimas_linhas=100000):\n",
        "    \"\"\"\n",
        "    PASSO 1: Carrega e prepara os dados.\n",
        "    Objetivo: Ler o arquivo CSV e garantir que ele esteja em ordem.\n",
        "    \"\"\"\n",
        "    # Carrega os dados do arquivo CSV\n",
        "    df = pd.read_csv(caminho_arquivo)\n",
        "\n",
        "    # Evita o PC de explodir com muitos dados\n",
        "    df = df.iloc[-ultimas_linhas:]\n",
        "\n",
        "    # Garante que a coluna 'timestamp' seja do tipo data e ordena os dados\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values('timestamp')\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Remove colunas com nomes \"Unnamed\".\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    print(f\"Dados carregados: {df.shape[0]} registros.\")\n",
        "    print(df.head())\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "d7e9b3f1",
      "metadata": {
        "id": "d7e9b3f1"
      },
      "outputs": [],
      "source": [
        "def preparar_features(df, df_btc=None, df_eth=None):\n",
        "    \"\"\"\n",
        "    PASSO 2: Preparar os Dados para o Modelo.\n",
        "    Objetivo: Transformar os dados brutos em um formato que a rede neural entenda (features e target).\n",
        "    \"\"\"\n",
        "    # --- Calculo dos Indicadores ---\n",
        "    df = df.copy()\n",
        "\n",
        "    if df_btc is not None:\n",
        "        btc_features = pd.DataFrame(index=df_btc.index)\n",
        "        btc_features['btc_close_pct'] = np.log1p(df_btc['close'].pct_change())\n",
        "        btc_features['btc_volume_pct'] = np.log1p(df_btc['volume'].pct_change())\n",
        "        btc_features['btc_price_trend'] = df_btc['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "        df = df.join(btc_features)\n",
        "        df['btc_correlation'] = df['close'].rolling(10).corr(df_btc['close'])\n",
        "\n",
        "    if df_eth is not None:\n",
        "        eth_features = pd.DataFrame(index=df_eth.index)\n",
        "        eth_features['eth_close_pct'] = np.log1p(df_eth['close'].pct_change())\n",
        "        eth_features['eth_volume_pct'] = np.log1p(df_eth['volume'].pct_change())\n",
        "        eth_features['eth_price_trend'] = df_eth['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "        df = df.join(eth_features)\n",
        "        df['eth_correlation'] = df['close'].rolling(10).corr(df_eth['close'])\n",
        "\n",
        "    # --- 1. Variações e transformações básicas ---\n",
        "    df['close_pct'] = np.log1p(df['close'].pct_change())\n",
        "    df['volume_pct'] = np.log1p(df['volume'].pct_change())\n",
        "\n",
        "    # --- 2. Médias móveis e suas diferenças ---\n",
        "    df['sma_10'] = df['close'].rolling(10).mean()\n",
        "    df['sma_30'] = df['close'].rolling(30).mean()\n",
        "    df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
        "    df['ema_30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
        "    df['sma_10_vs_sma_30'] = df['sma_10'] - df['sma_30']\n",
        "    df['price_vs_sma30'] = df['close'] - df['sma_30']\n",
        "\n",
        "    # --- 3. Indicadores técnicos comuns ---\n",
        "    def calc_rsi(series, period):\n",
        "        delta = series.diff()\n",
        "        gain = delta.clip(lower=0).rolling(period).mean()\n",
        "        loss = (-delta.clip(upper=0)).rolling(period).mean()\n",
        "        rs = gain / (loss + 1e-10)\n",
        "        return 100 - 100 / (1 + rs)\n",
        "\n",
        "    for p in [7, 14, 21]:\n",
        "        df[f'rsi_{p}'] = calc_rsi(df['close'], p)\n",
        "\n",
        "    def macd(series, short=12, long=26, signal=9):\n",
        "        exp1 = series.ewm(span=short, adjust=False).mean()\n",
        "        exp2 = series.ewm(span=long, adjust=False).mean()\n",
        "        macd_line = exp1 - exp2\n",
        "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
        "        return macd_line, signal_line\n",
        "\n",
        "    df['macd'], df['macd_signal'] = macd(df['close'])\n",
        "    df['macd_diff'] = df['macd'] - df['macd_signal']\n",
        "\n",
        "    def bollinger(series, window=20, std_dev=2):\n",
        "        ma = series.rolling(window).mean()\n",
        "        std = series.rolling(window).std()\n",
        "        upper = ma + std_dev * std\n",
        "        lower = ma - std_dev * std\n",
        "        return upper, lower\n",
        "\n",
        "    df['bb_upper'], df['bb_lower'] = bollinger(df['close'])\n",
        "    df['bb_upper_diff'] = df['bb_upper'] - df['close']\n",
        "    df['bb_lower_diff'] = df['close'] - df['bb_lower']\n",
        "\n",
        "    def atr(df, window=14):\n",
        "        tr = pd.concat([\n",
        "            df['high'] - df['low'],\n",
        "            abs(df['high'] - df['close'].shift()),\n",
        "            abs(df['low'] - df['close'].shift())\n",
        "        ], axis=1).max(axis=1)\n",
        "        return tr.rolling(window).mean()\n",
        "\n",
        "    df['atr_14'] = atr(df)\n",
        "\n",
        "    # --- 4. Osciladores ---\n",
        "    def stochastic(df, k_window=14, d_window=3):\n",
        "        low_min = df['low'].rolling(k_window).min()\n",
        "        high_max = df['high'].rolling(k_window).max()\n",
        "        k = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
        "        d = k.rolling(d_window).mean()\n",
        "        return k, d\n",
        "\n",
        "    df['so_k'], df['so_d'] = stochastic(df)\n",
        "\n",
        "    def williams_r(df, window=14):\n",
        "        low_min = df['low'].rolling(window).min()\n",
        "        high_max = df['high'].rolling(window).max()\n",
        "        return -100 * (high_max - df['close']) / (high_max - low_min + 1e-10)\n",
        "\n",
        "    df['wpr_14'] = williams_r(df)\n",
        "\n",
        "    # --- 5. Indicadores de volume ---\n",
        "    # OTIMIZAÇÃO: Lógica do OBV vetorizada para muito mais performance\n",
        "    price_direction = np.sign(df['close'].diff()).fillna(0)\n",
        "    df['obv'] = (price_direction * df['volume']).cumsum()\n",
        "\n",
        "    df['volume_trend'] = df['volume'].pct_change().rolling(10).mean()\n",
        "    df['price_trend'] = df['close'].pct_change().rolling(10).mean()\n",
        "\n",
        "    # --- 6. Volatilidade e desvio padrão ---\n",
        "    for w in [10, 20, 40]:\n",
        "        df[f'std_{w}'] = df['close'].rolling(w).std()\n",
        "        df[f'volatility_{w}'] = df['close'].pct_change().rolling(w).std()\n",
        "\n",
        "    # --- 7. Lags de indicadores principais ---\n",
        "    cols_to_lag = [\n",
        "        'close_pct', 'rsi_7', 'rsi_14', 'rsi_21', 'macd', 'macd_signal',\n",
        "        'btc_close_pct', 'btc_volume_pct', 'btc_price_trend', 'btc_correlation',\n",
        "        'eth_close_pct', 'eth_volume_pct', 'eth_price_trend', 'eth_correlation'\n",
        "    ]\n",
        "\n",
        "    for col in cols_to_lag:\n",
        "        if col in df.columns:  # Apenas cria o lag se a coluna existir no df final\n",
        "            df[f'lag_{col}_1'] = df[col].shift(1)\n",
        "            df[f'lag_{col}_2'] = df[col].shift(2)\n",
        "            df[f'lag_{col}_3'] = df[col].shift(3)\n",
        "            df[f'lag_{col}_4'] = df[col].shift(4)\n",
        "            df[f'lag_{col}_5'] = df[col].shift(5)\n",
        "\n",
        "    cols_to_diff = [\n",
        "        'close', 'sma_10', 'sma_30', 'ema_10', 'ema_30',\n",
        "        'bb_upper', 'bb_lower', 'macd', 'macd_signal', 'obv',\n",
        "        'btc_close_pct', 'btc_volume_pct', 'btc_price_trend', 'rsi_7',\n",
        "        'rsi_14', 'rsi_21', 'eth_close_pct', 'eth_volume_pct', 'eth_price_trend'\n",
        "    ]\n",
        "\n",
        "    for col in cols_to_diff:\n",
        "        if col in df.columns:\n",
        "            df[f'diff_{col}_1'] = df[col].diff(1)\n",
        "            df[f'diff_{col}_2'] = df[col].diff(2)\n",
        "            df[f'diff_{col}_3'] = df[col].diff(3)\n",
        "            df[f'diff_{col}_4'] = df[col].diff(4)\n",
        "            df[f'diff_{col}_5'] = df[col].diff(5)\n",
        "\n",
        "    # --- 8. Finalização ---\n",
        "    df.dropna(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "e9b2a8d1",
      "metadata": {
        "id": "e9b2a8d1"
      },
      "outputs": [],
      "source": [
        "def definir_features_e_alvo(df):\n",
        "    \"\"\"Define as colunas de features e a coluna alvo, e retorna os dados brutos X e y.\"\"\"\n",
        "    FEATURE_COLUMNS = df.columns.tolist()\n",
        "    # Remove a coluna alvo do conjunto de features\n",
        "    FEATURE_COLUMNS.remove('timestamp')  # Timestamp não é uma feature\n",
        "    FEATURE_COLUMNS.remove('close')  # A coluna alvo não deve ser uma feature\n",
        "    FEATURE_COLUMNS.remove('volume')  # Volume não é uma feature direta\n",
        "    FEATURE_COLUMNS.remove('high')  # High não é uma feature direta\n",
        "    FEATURE_COLUMNS.remove('low')  # Low não é uma feature direta\n",
        "    FEATURE_COLUMNS.remove('open')  # Open não é uma feature direta\n",
        "\n",
        "    TARGET_COLUMN = 'close'\n",
        "\n",
        "    # O alvo (y) será binário: 1 se o preço subir, 0 se não\n",
        "    # df[TARGET_COLUMN].shift(-1) pega o preço de fechamento do *próximo* período\n",
        "    y = (df[TARGET_COLUMN].shift(-1) > df[TARGET_COLUMN]).astype(int)\n",
        "\n",
        "    # As features (X) serão os dados da FEATURE_COLUMNS\n",
        "    X_raw = df[FEATURE_COLUMNS]\n",
        "\n",
        "    num_features = len(FEATURE_COLUMNS)\n",
        "\n",
        "    print(f\"Quantidade de features definidas: {num_features}\")\n",
        "    print(f\"Shape dos dados brutos: {X_raw.shape}\")\n",
        "    print(f\"Valores mínimos {X_raw.min()}\")\n",
        "    print(f\"Valores máximos {X_raw.max()}\")\n",
        "\n",
        "    return X_raw, y, FEATURE_COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "f5c1a0e8",
      "metadata": {
        "id": "f5c1a0e8"
      },
      "outputs": [],
      "source": [
        "def normalizar_dados(X_raw):\n",
        "    \"\"\"Normaliza os dados de features.\"\"\"\n",
        "    # --- Normalização dos Dados ---\n",
        "\n",
        "    # Pipeline de normalização: Log transformation + Tanh transformation + Vector normalization\n",
        "\n",
        "    # Função personalizada para transformação logaritma\n",
        "    def log_transform(X):\n",
        "        \"\"\"Aplica transformação logaritma aos dados\"\"\"\n",
        "        return np.log(X + 1)  # Adiciona 1 para evitar log(0)\n",
        "\n",
        "    # Função personalizada para transformação Tanh\n",
        "    def tanh_transform(X):\n",
        "        \"\"\"Aplica transformação tanh aos dados para reduzir outliers\"\"\"\n",
        "        return np.tanh(X)\n",
        "\n",
        "    # # Criar o pipeline de normalização\n",
        "    # scaler = Pipeline([\n",
        "    #     # ('log', FunctionTransformer(log_transform, validate=True)),\n",
        "    #     ('tanh', FunctionTransformer(tanh_transform, validate=True)),\n",
        "    #     ('normalize', Normalizer(norm='l2')),  # Normalização L2 (vetorial)\n",
        "    #     # ('minmax', MinMaxScaler(feature_range=(-1, 1)))\n",
        "    # ])\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    X_scaled = scaler.fit_transform(X_raw)\n",
        "\n",
        "    print(f\"Dados normalizados. Shape: {X_scaled.shape}\")\n",
        "    print(f\"Min: {X_scaled.min():.6f}, Max: {X_scaled.max():.6f}\")\n",
        "    # print(f\"Norma L2 das primeiras 5 amostras: {np.linalg.norm(X_scaled[:5], axis=1)}\")\n",
        "    return X_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "a1b2c3d4",
      "metadata": {
        "id": "a1b2c3d4"
      },
      "outputs": [],
      "source": [
        "def criar_sequencias(X_scaled, y, sequence_length):\n",
        "    \"\"\"Cria sequências de dados para o modelo.\"\"\"\n",
        "    X_sequences, y_sequences = [], []\n",
        "\n",
        "    # Verificar o tamanho correto para evitar index out of bounds\n",
        "    max_index = min(len(X_scaled), len(y)) - sequence_length\n",
        "\n",
        "    print(f\"Tamanho de X_scaled: {len(X_scaled)}\")\n",
        "    print(f\"Tamanho de y: {len(y)}\")\n",
        "    print(f\"Sequence length: {sequence_length}\")\n",
        "    print(f\"Max index para loop: {max_index}\")\n",
        "\n",
        "    for i in range(max_index):\n",
        "        X_sequences.append(X_scaled[i : i + sequence_length])\n",
        "        y_sequences.append(y[i + sequence_length - 1]) # O alvo corresponde ao final da sequência\n",
        "\n",
        "    X = np.array(X_sequences)\n",
        "    y = np.array(y_sequences)\n",
        "\n",
        "    print(f\"Sequências criadas. X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "e5f6a7b8",
      "metadata": {
        "id": "e5f6a7b8"
      },
      "outputs": [],
      "source": [
        "def dividir_dados(X, y, test_data=None, class_weight_multiplier=1.0):\n",
        "    \"\"\"Divide os dados em conjuntos de treino e teste usando TimeSeriesSplit.\"\"\"\n",
        "    # --- Divisão em Treino e Teste ---\n",
        "    # Divisão dos dados em treino e teste\n",
        "    tscv = TimeSeriesSplit(n_splits=5, test_size=test_data)\n",
        "    for train_index, test_index in tscv.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Tentativa de balancear as classes, evitando a mesma reposta sempre\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Aumenta o peso da classe minoritária\n",
        "    if class_weight_dict[0] < class_weight_dict[1]:\n",
        "        class_weight_dict[0] *= class_weight_multiplier\n",
        "    else:\n",
        "        class_weight_dict[1] *= class_weight_multiplier\n",
        "\n",
        "\n",
        "    print(f\"Amostras de treino: {len(X_train)}\")\n",
        "    print(f\"Amostras de teste: {len(X_test)}\")\n",
        "    print(f\"Distribuição do target no treino: {np.bincount(y_train)}\")\n",
        "    print(f\"Distribuição do target no teste: {np.bincount(y_test)}\")\n",
        "    return X_train, X_test, y_train, y_test, class_weight_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "c9d8e7f6",
      "metadata": {
        "id": "c9d8e7f6"
      },
      "outputs": [],
      "source": [
        "def binary_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\"Função de perda focal binária.\n",
        "    Objetivo: Aumentar o peso das classes minoritárias e reduzir o impacto das classes majoritárias.\n",
        "    \"\"\"\n",
        "    # Função que retorna outra função para calcular a perda focal binária\n",
        "    def binary_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"Função de perda focal binária.\n",
        "        Objetivo: Aumentar o peso das classes minoritárias e reduzir o impacto das classes majoritárias.\n",
        "        \"\"\"\n",
        "        # Se o rótulo for 1, pega a predição, senão usa 1 (neutro)\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        # Se o rótulo for 0, pega a predição, senão usa 0 (neutro)\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\n",
        "        # Evita problemas com log(0) usando um valor muito pequeno\n",
        "        epsilon = K.epsilon()\n",
        "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
        "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Calcula o erro para a classe 1 com peso alpha e penalidade (1 - pt)^gamma\n",
        "        loss_1 = -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))\n",
        "        # Calcula o erro para a classe 0 com peso (1 - alpha) e penalidade pt^gamma\n",
        "        loss_0 = -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "\n",
        "        # Soma total das perdas das duas classes\n",
        "        return loss_1 + loss_0\n",
        "\n",
        "    return binary_focal_loss_fixed\n",
        "\n",
        "\n",
        "def construir_e_compilar_modelo(sequence_length, num_features, loss_function='binary_crossentropy'):\n",
        "    \"\"\"\n",
        "    PASSO 3: Constrói e compila o modelo MLP.\n",
        "    Objetivo: Definir a arquitetura da nossa rede neural e compilá-la.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adiciona a camada de entrada com o formato correto\n",
        "    model.add(Input(shape=(sequence_length, num_features)))\n",
        "\n",
        "    # Camada de Entrada: Uso do Conv1D para capturar padrões temporais\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Camada de Pooling: Reduz a dimensionalidade e extrai características importantes\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Achata a saída do Conv1D para ser usada nas camadas densas\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "\n",
        "    # 1ª Camada Oculta: Primeira camada que aprende os padrões\n",
        "    model.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # PReLU é uma ativação que pode aprender a inclinação\n",
        "    model.add(PReLU())\n",
        "\n",
        "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
        "    model.add(Dropout(rate=0.1))\n",
        "\n",
        "    # 2ª Camada Oculta: Segunda camada para aprender padrões mais complexos\n",
        "    model.add(Dense(units=32, kernel_regularizer=regularizers.l2(0.0001)))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # PReLU é uma ativação que pode aprender a inclinação\n",
        "    model.add(PReLU())\n",
        "\n",
        "    # Camada de Dropout: Ajuda a prevenir overfitting\n",
        "    model.add(Dropout(rate=0.1))\n",
        "\n",
        "    # Camada de Saída: Gera a previsão final\n",
        "    model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    # --- Compilação do Modelo ---\n",
        "    # O modelo é compilado com o otimizador Adam e a função de perda binary_crossentropy, adequada para problemas de classificação binária\n",
        "    if loss_function == 'binary_focal_crossentropy':\n",
        "        loss = binary_focal_loss(gamma=2., alpha=.25)\n",
        "    else:\n",
        "        loss = loss_function\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Mostra um resumo da arquitetura do modelo\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "b7a6d5c4",
      "metadata": {
        "id": "b7a6d5c4"
      },
      "outputs": [],
      "source": [
        "def treinar_modelo(model, X_train, y_train, X_test, y_test, class_weight_dict):\n",
        "    \"\"\"\n",
        "    PASSO 4: Treina o modelo.\n",
        "    Objetivo: Alimentar o modelo com os dados de treino para que ele aprenda.\n",
        "    \"\"\"\n",
        "    # Callbacks para otimizar o treinamento\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=200,  # epochs: Quantas vezes o modelo verá todo o conjunto de dados de treino.\n",
        "        batch_size=64,  # batch_size: Quantas amostras o modelo vê antes de atualizar seus pesos.\n",
        "        validation_data=(X_test, y_test),  # Dados para validar o modelo a cada época.\n",
        "        class_weight=class_weight_dict,  # Pesos das classes para lidar com desbalanceamento\n",
        "        callbacks=callbacks,  # Callbacks para otimização do treinamento\n",
        "        verbose=1 # Mostra uma barra de progresso.\n",
        "    )\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "a5b4c3d2",
      "metadata": {
        "id": "a5b4c3d2"
      },
      "outputs": [],
      "source": [
        "def avaliar_modelo(model, X_test, y_test, df_features_teste, threshold=0.5):\n",
        "    \"\"\"\n",
        "    PASSO 5: Avalia o modelo.\n",
        "    Objetivo: Verificar o quão bem o modelo se saiu nos dados de teste,\n",
        "              com a opção de aplicar um filtro condicional.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- DISTRIBUIÇÃO DE CLASSES NO CONJUNTO DE TESTE ---\")\n",
        "    unique, counts = np.unique(y_test, return_counts=True)\n",
        "    for label, count in zip(unique, counts):\n",
        "        print(f\"Classe {label} ({'Sobe' if label == 1 else 'Não Sobe'}): {count} amostras ({count / len(y_test) * 100:.2f}%)\")\n",
        "\n",
        "    # Previsões globais (sem filtro)\n",
        "    print(\"\\n--- AVALIAÇÃO GLOBAL ---\")\n",
        "    probas_global = model.predict(X_test)\n",
        "    preds_global = (probas_global > threshold).astype(int)\n",
        "\n",
        "    acc_global = accuracy_score(y_test, preds_global)\n",
        "    prec_global = precision_score(y_test, preds_global, zero_division=0)\n",
        "    recall_global = recall_score(y_test, preds_global, zero_division=0)\n",
        "    f1_global = f1_score(y_test, preds_global, zero_division=0)\n",
        "    auc_global = roc_auc_score(y_test, probas_global)\n",
        "\n",
        "    print(f\"Acurácia: {acc_global * 100:.2f}%\")\n",
        "    print(f\"Precisão: {prec_global * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall_global * 100:.2f}%\")\n",
        "    print(f\"F1-score: {f1_global * 100:.2f}%\")\n",
        "    print(f\"AUC/ROC: {auc_global:.4f}\")\n",
        "\n",
        "    print(\"\\nRelatório de Classificação (Global):\")\n",
        "    print(classification_report(y_test, preds_global, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
        "\n",
        "    # Matriz de confusão (global)\n",
        "    cm_global = confusion_matrix(y_test, preds_global)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_global, annot=True, fmt='d', cmap='Purples', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusão (Global)')\n",
        "    plt.show()\n",
        "\n",
        "    # Otimização da condição\n",
        "    step_config = {\n",
        "        'rsi_7': 2,\n",
        "        'rsi_14': 2,\n",
        "        'so_d': 1,\n",
        "        'macd_diff': 0.02\n",
        "    }\n",
        "\n",
        "    params_to_optimize = {}\n",
        "    features_to_optimize = ['rsi_7', 'rsi_14', 'so_d', 'macd_diff']\n",
        "\n",
        "    for feature in features_to_optimize:\n",
        "        if feature not in df_features_teste.columns:\n",
        "            print(f\"Aviso: Feature '{feature}' não encontrada. Pulando otimização para esta feature.\")\n",
        "            continue\n",
        "\n",
        "        # Calcula os quartis e os limites (mínimo e máximo) da feature\n",
        "        q1 = df_features_teste[feature].quantile(0.25)\n",
        "        q3 = df_features_teste[feature].quantile(0.75)\n",
        "        min_val = df_features_teste[feature].min()\n",
        "        max_val = df_features_teste[feature].max()\n",
        "        step = step_config.get(feature, 1) # Usa o passo configurado ou 1 como padrão\n",
        "\n",
        "        # Define o range de busca para o limite superior (acima do 3º quartil)\n",
        "        # Adicionamos 'step' ao valor final porque np.arange é exclusivo\n",
        "        range_sup = [q3, max_val + step, step]\n",
        "\n",
        "        # Define o range de busca para o limite inferior (abaixo do 1º quartil)\n",
        "        range_inf = [min_val, q1 + step, step]\n",
        "\n",
        "        # Garante que os limites não se sobreponham e que o range seja válido\n",
        "        if range_sup[0] >= range_sup[1]: continue # Pula se Q3 for igual ou maior que o máximo\n",
        "        if range_inf[0] >= range_inf[1]: continue # Pula se o mínimo for igual ou maior que Q1\n",
        "\n",
        "        params_to_optimize[feature] = (range_sup, range_inf)\n",
        "\n",
        "        print(f\"\\nFeature: '{feature}'\")\n",
        "        print(f\"  - Intervalo de busca p/ Limite Superior (>= Q3): de {q3:.2f} a {max_val:.2f} (passo {step})\")\n",
        "        print(f\"  - Intervalo de busca p/ Limite Inferior (<= Q1): de {min_val:.2f} a {q1:.2f} (passo {step})\")\n",
        "\n",
        "    if not params_to_optimize:\n",
        "        print(\"\\nNão foi possível definir parâmetros de otimização. Avaliação condicional não será realizada.\")\n",
        "        return\n",
        "\n",
        "    # A chamada para otimizar_condicao usa os parâmetros dinâmicos\n",
        "    condicao_otima = otimizar_condicao(\n",
        "        model,\n",
        "        df_features_teste,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        params_to_optimize,\n",
        "        min_amostras=100,\n",
        "        metrica='f1_score'\n",
        "    )\n",
        "\n",
        "    if condicao_otima is None or not condicao_otima.get('condicao'):\n",
        "        print(\"\\nNenhuma condição de filtro foi otimizada. Avaliação condicional não será realizada.\")\n",
        "        return\n",
        "\n",
        "    feature_name = condicao_otima['feature']\n",
        "    limite_superior = condicao_otima['limite_superior']\n",
        "    limite_inferior = condicao_otima['limite_inferior']\n",
        "\n",
        "    condicao = (df_features_teste[feature_name] > limite_superior) | (df_features_teste[feature_name] < limite_inferior)\n",
        "    filtro = condicao.iloc[-len(y_test):].values\n",
        "\n",
        "    print(f\"\\nA condição otimizada '{condicao_otima['condicao']}' será aplicada a {sum(filtro)} de {len(y_test)} amostras de teste ({sum(filtro)/len(y_test)*100:.2f}%).\")\n",
        "\n",
        "    if sum(filtro) == 0:\n",
        "        print(\"Nenhuma amostra de teste atendeu à condição especificada.\")\n",
        "        return\n",
        "\n",
        "    X_test_filtrado = X_test[filtro]\n",
        "    y_test_filtrado = y_test[filtro]\n",
        "\n",
        "    probas_filtrado = model.predict(X_test_filtrado)\n",
        "    preds_filtrado = (probas_filtrado > threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
        "    prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    recall = recall_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0)\n",
        "    auc = roc_auc_score(y_test_filtrado, probas_filtrado)\n",
        "\n",
        "    print(f\"\\n--- AVALIAÇÃO COM FILTRO OTIMIZADO ({condicao_otima['condicao']}) ---\")\n",
        "    print(f\"Acurácia: {acc * 100:.2f}%\")\n",
        "    print(f\"Precisão: {prec * 100:.2f}%\")\n",
        "    print(f\"Recall: {recall * 100:.2f}%\")\n",
        "    print(f\"F1-score: {f1 * 100:.2f}%\")\n",
        "    print(f\"AUC/ROC: {auc:.4f}\")\n",
        "\n",
        "    print(\"\\nRelatório de Classificação (Filtrado):\")\n",
        "    print(classification_report(y_test_filtrado, preds_filtrado, target_names=['Não Sobe', 'Sobe'], zero_division=0))\n",
        "\n",
        "    cm = confusion_matrix(y_test_filtrado, preds_filtrado)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
        "    plt.xlabel('Predito')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title(f\"Matriz de Confusão ({condicao_otima['condicao']})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def otimizar_condicao(model, df_features_teste, X_test, y_test, params_to_optimize, min_amostras=100, metrica='f1_score'):\n",
        "    \"\"\"\n",
        "    Passo 5.1: Otimiza a condição de filtro para avaliação do modelo.\n",
        "\n",
        "    Objetivo: Encontrar a melhor combinação de indicador e limites (thresholds)\n",
        "              que maximize uma métrica de desempenho específica.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- OTIMIZAÇÃO DE CONDIÇÃO (Métrica: {metrica}, Mín. Amostras: {min_amostras}) ---\")\n",
        "\n",
        "    melhores_resultados = {\n",
        "        'pontuacao': -1.0,\n",
        "        'precisao': 0.0,\n",
        "        'f1_score': 0.0,\n",
        "        'acuracia': 0.0,\n",
        "        'num_amostras': 0,\n",
        "        'condicao': None,\n",
        "        'feature': None,\n",
        "        'limite_superior': None,\n",
        "        'limite_inferior': None\n",
        "    }\n",
        "    resultados_testados = []\n",
        "\n",
        "    # Iterar sobre cada feature e seus respectivos ranges de busca\n",
        "    for feature, (range_sup, range_inf) in params_to_optimize.items():\n",
        "        # Desempacotar início, fim e passo para os limites superior e inferior\n",
        "        start_sup, stop_sup, step_sup = range_sup\n",
        "        start_inf, stop_inf, step_inf = range_inf\n",
        "\n",
        "        # Gerar os limites (thresholds) a serem testados usando np.arange\n",
        "        limites_superiores = np.arange(start_sup, stop_sup, step_sup)\n",
        "        limites_inferiores = np.arange(start_inf, stop_inf, step_inf)\n",
        "\n",
        "        # Iterar sobre todas as combinações de limites\n",
        "        for sup in limites_superiores:\n",
        "            for inf in limites_inferiores:\n",
        "                # Cria a condição de filtro com os parâmetros atuais\n",
        "                condicao = (df_features_teste[feature] > sup) | (df_features_teste[feature] < inf)\n",
        "                filtro = condicao.iloc[-len(y_test):].values\n",
        "                num_amostras_filtradas = sum(filtro)\n",
        "\n",
        "                if num_amostras_filtradas < min_amostras:\n",
        "                    continue\n",
        "\n",
        "                X_test_filtrado = X_test[filtro]\n",
        "                y_test_filtrado = y_test[filtro]\n",
        "\n",
        "                # Adicionado verbose=0 para não poluir a saída do console durante a otimização\n",
        "                probas_filtrado = model.predict(X_test_filtrado, verbose=0)\n",
        "                preds_filtrado = (probas_filtrado > 0.5).astype(int)\n",
        "\n",
        "                acc = accuracy_score(y_test_filtrado, preds_filtrado)\n",
        "                prec = precision_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
        "                f1 = f1_score(y_test_filtrado, preds_filtrado, zero_division=0, pos_label=1)\n",
        "\n",
        "                resultados_testados.append({\n",
        "                    'feature': feature,\n",
        "                    'limite_superior': round(sup, 4),\n",
        "                    'limite_inferior': round(inf, 4),\n",
        "                    'precisao': prec,\n",
        "                    'f1_score': f1,\n",
        "                    'acuracia': acc,\n",
        "                    'num_amostras': num_amostras_filtradas\n",
        "                })\n",
        "\n",
        "                metric_map = {\n",
        "                    'acuracia': acc,\n",
        "                    'precisao': prec,\n",
        "                    'f1_score': f1\n",
        "                }\n",
        "                pontuacao_atual = metric_map.get(metrica, 0.0)\n",
        "\n",
        "                if pontuacao_atual > melhores_resultados['pontuacao']:\n",
        "                    melhores_resultados.update({\n",
        "                        'pontuacao': pontuacao_atual,\n",
        "                        'precisao': prec,\n",
        "                        'f1_score': f1,\n",
        "                        'acuracia': acc,\n",
        "                        'num_amostras': num_amostras_filtradas,\n",
        "                        'condicao': f\"{feature} > {sup:.2f} ou {feature} < {inf:.2f}\",\n",
        "                        'feature': feature,\n",
        "                        'limite_superior': sup,\n",
        "                        'limite_inferior': inf\n",
        "                    })\n",
        "\n",
        "    if not resultados_testados:\n",
        "        print(\"Nenhuma combinação de parâmetros resultou em amostras suficientes para avaliação.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n--- MELHOR CONDIÇÃO ENCONTRADA ---\")\n",
        "    if melhores_resultados['condicao']:\n",
        "        print(f\"Condição: {melhores_resultados['condicao']}\")\n",
        "        print(f\"Número de Amostras: {melhores_resultados['num_amostras']} ({melhores_resultados['num_amostras'] / len(y_test) * 100:.2f}%)\")\n",
        "        print(f\"Métrica Otimizada ({metrica}): {melhores_resultados['pontuacao'] * 100:.2f}%\")\n",
        "        print(f\"\\n--- Métricas de Desempenho ---\")\n",
        "        print(f\"Acurácia: {melhores_resultados['acuracia'] * 100:.2f}%\")\n",
        "        print(f\"Precisão (Classe 'Sobe'): {melhores_resultados['precisao'] * 100:.2f}%\")\n",
        "        print(f\"F1-score (Classe 'Sobe'): {melhores_resultados['f1_score'] * 100:.2f}%\")\n",
        "    else:\n",
        "        print(\"Não foi possível encontrar uma condição ótima com os parâmetros fornecidos.\")\n",
        "        return None\n",
        "\n",
        "    # Exibir os top 5 resultados em um DataFrame para análise\n",
        "    df_resultados = pd.DataFrame(resultados_testados)\n",
        "    df_resultados = df_resultados.sort_values(by=metrica, ascending=False)\n",
        "    print(\"\\n--- Top 5 Melhores Condições ---\")\n",
        "    print(df_resultados.head())\n",
        "\n",
        "    return melhores_resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "e1f2a3b4",
      "metadata": {
        "id": "e1f2a3b4"
      },
      "outputs": [],
      "source": [
        "def visualizar_treinamento(history):\n",
        "    \"\"\"\n",
        "    PASSO 6: Visualiza o treinamento.\n",
        "    Objetivo: Plotar gráficos para ver se o modelo aprendeu bem ou se teve overfitting.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Gráfico da Acurácia\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
        "    plt.title('Acurácia ao Longo das Épocas')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "\n",
        "    # Gráfico da Perda (Loss)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Perda de Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
        "    plt.title('Perda ao Longo das Épocas')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Perda')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "c5d6e7f8",
      "metadata": {
        "id": "c5d6e7f8"
      },
      "outputs": [],
      "source": [
        "def salvar_modelo_e_scaler(model, scaler):\n",
        "    \"\"\"\n",
        "    PASSO 7: Salva o modelo e o normalizador.\n",
        "    Objetivo: Salvar o modelo treinado e o normalizador para uso futuro.\n",
        "    \"\"\"\n",
        "    # Por enquanto não é necessário salvar o modelo\n",
        "\n",
        "    MODEL_PATH = \"mlp_basic_model.h5\"\n",
        "    SCALER_PATH = \"basic_scaler.pkl\"\n",
        "\n",
        "    model.save(MODEL_PATH)\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "\n",
        "    print(f\"Modelo salvo em: {MODEL_PATH}\")\n",
        "    print(f\"Normalizador salvo em: {SCALER_PATH}\")\n",
        "\n",
        "    print(\"Função 'salvar_modelo_e_scaler' chamada, mas a funcionalidade está comentada conforme o original.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9b8c7d6",
      "metadata": {
        "id": "a9b8c7d6"
      },
      "source": [
        "## Execução Principal (Main)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b249bf71",
      "metadata": {
        "id": "b249bf71"
      },
      "source": [
        "### Definições Globais e Carregamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "e44f45ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44f45ac",
        "outputId": "40c2db26-9a06-4a87-9393-ea12a3014892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dados carregados: 70080 registros.\n",
            "            timestamp   open   high    low  close     volume\n",
            "0 2023-07-20 15:30:00  26.20  26.21  25.75  25.91  121467.56\n",
            "1 2023-07-20 15:45:00  25.90  25.91  25.57  25.69  191787.08\n",
            "2 2023-07-20 16:00:00  25.69  25.77  25.56  25.66  106474.05\n",
            "3 2023-07-20 16:15:00  25.67  25.67  25.21  25.37  170476.73\n",
            "4 2023-07-20 16:30:00  25.38  25.47  25.33  25.42   75885.50\n",
            "Dados carregados: 70080 registros.\n",
            "            timestamp      open      high       low     close     volume\n",
            "0 2023-07-20 15:30:00  29842.00  29848.50  29788.89  29823.49  464.43177\n",
            "1 2023-07-20 15:45:00  29823.42  29823.42  29766.65  29810.18  374.59252\n",
            "2 2023-07-20 16:00:00  29810.17  29872.71  29774.50  29840.37  461.98819\n",
            "3 2023-07-20 16:15:00  29840.38  29845.13  29738.01  29761.99  523.28591\n",
            "4 2023-07-20 16:30:00  29762.00  29781.59  29715.00  29742.54  412.62128\n",
            "Dados carregados: 70080 registros.\n",
            "            timestamp     open     high      low    close     volume\n",
            "0 2023-07-20 15:30:00  1892.80  1893.25  1887.21  1889.79  4842.1955\n",
            "1 2023-07-20 15:45:00  1889.80  1889.80  1885.01  1888.61  5295.4780\n",
            "2 2023-07-20 16:00:00  1888.60  1892.05  1885.80  1889.66  3666.8912\n",
            "3 2023-07-20 16:15:00  1889.66  1889.98  1882.77  1885.99  6010.2168\n",
            "4 2023-07-20 16:30:00  1885.98  1887.99  1882.75  1885.72  3679.8005\n"
          ]
        }
      ],
      "source": [
        "# --- Definições Globais ---\n",
        "CAMINHO_ARQUIVO = r\"../../data/fechamentos/SOLUSDT_15m_data.csv\"\n",
        "SEQUENCE_LENGTH = 36 # Quantos períodos olharemos para trás para prever o próximo\n",
        "\n",
        "# --- PASSOS DO PROCESSO ---\n",
        "# Passo 0: Puxar dados de outras criptos para comparação de comportamento\n",
        "CAMINHO_ARQUIVO_BTC = r\"../../data/fechamentos/BTCUSDT_15m_data.csv\"\n",
        "CAMINHO_ARQUIVO_ETH = r\"../../data/fechamentos/ETHUSDT_15m_data.csv\"\n",
        "\n",
        "# PASSO 1: Carregar os dados\n",
        "df_original = carregar_dados(CAMINHO_ARQUIVO)\n",
        "df_comparacao_btc = carregar_dados(CAMINHO_ARQUIVO_BTC)\n",
        "df_comparacao_eth = carregar_dados(CAMINHO_ARQUIVO_ETH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7bf2ed",
      "metadata": {
        "id": "1e7bf2ed"
      },
      "source": [
        "### Preparar Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "fd257ac2",
      "metadata": {
        "id": "fd257ac2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'lag_{col}_1'] = df[col].shift(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'lag_{col}_2'] = df[col].shift(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'lag_{col}_3'] = df[col].shift(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'lag_{col}_4'] = df[col].shift(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'lag_{col}_5'] = df[col].shift(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_1'] = df[col].diff(1)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_2'] = df[col].diff(2)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_3'] = df[col].diff(3)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_4'] = df[col].diff(4)\n",
            "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_19492\\1595318990.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'diff_{col}_5'] = df[col].diff(5)\n"
          ]
        }
      ],
      "source": [
        "# PASSO 2: Preparar features\n",
        "df_features = preparar_features(df_original, df_comparacao_btc, df_comparacao_eth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca205a2e",
      "metadata": {
        "id": "ca205a2e"
      },
      "source": [
        "### Definir Features e Alvo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "e2dddd42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2dddd42",
        "outputId": "098d1fdc-8204-4aed-9f64-f12e937060e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantidade de features definidas: 204\n",
            "Shape dos dados brutos: (70040, 204)\n",
            "Valores mínimos btc_close_pct            -0.094087\n",
            "btc_volume_pct           -2.381147\n",
            "btc_price_trend          -0.009931\n",
            "btc_correlation          -0.904546\n",
            "eth_close_pct            -0.098436\n",
            "                            ...   \n",
            "diff_eth_price_trend_1   -0.009935\n",
            "diff_eth_price_trend_2   -0.011356\n",
            "diff_eth_price_trend_3   -0.013449\n",
            "diff_eth_price_trend_4   -0.013356\n",
            "diff_eth_price_trend_5   -0.014636\n",
            "Length: 204, dtype: float64\n",
            "Valores máximos btc_close_pct             0.053934\n",
            "btc_volume_pct            3.443955\n",
            "btc_price_trend           0.010580\n",
            "btc_correlation           0.999121\n",
            "eth_close_pct             0.092435\n",
            "                            ...   \n",
            "diff_eth_price_trend_1    0.010199\n",
            "diff_eth_price_trend_2    0.012972\n",
            "diff_eth_price_trend_3    0.013039\n",
            "diff_eth_price_trend_4    0.016103\n",
            "diff_eth_price_trend_5    0.016645\n",
            "Length: 204, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# PASSO 2.1: Definir features e alvo\n",
        "X_raw, y_raw, feature_columns = definir_features_e_alvo(df_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4536cb2",
      "metadata": {
        "id": "f4536cb2"
      },
      "source": [
        "### Normalizar Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607b5b42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607b5b42",
        "outputId": "826dc23e-1fb7-490f-975c-347a55c97dbc"
      },
      "outputs": [],
      "source": [
        "# PASSO 2.2: Normalizar dados\n",
        "X_scaled, scaler = normalizar_dados(X_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1ca07f",
      "metadata": {
        "id": "2c1ca07f"
      },
      "source": [
        "### Criar Sequências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cdf3249",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cdf3249",
        "outputId": "3a032160-e679-49d5-f1a5-3c1dd416fb54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamanho de X_scaled: 70040\n",
            "Tamanho de y: 70040\n",
            "Sequence length: 36\n",
            "Max index para loop: 70004\n",
            "Sequências criadas. X shape: (70004, 36, 204), y shape: (70004,)\n"
          ]
        }
      ],
      "source": [
        "# PASSO 2.3: Criar sequências\n",
        "X_seq, y_seq = criar_sequencias(X_scaled, y_raw, SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab485074",
      "metadata": {
        "id": "ab485074"
      },
      "source": [
        "### Dividir em Treino e Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48252a11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48252a11",
        "outputId": "f525f9ed-a97e-468f-9c0e-7f4743375773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amostras de treino: 58337\n",
            "Amostras de teste: 11667\n",
            "Distribuição do target no treino: [29499 28838]\n",
            "Distribuição do target no teste: [5814 5853]\n"
          ]
        }
      ],
      "source": [
        "# PASSO 2.4: Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test, class_weights = dividir_dados(X_seq, y_seq)\n",
        "inicio_teste = len(df_features) - len(X_test)\n",
        "df_features_teste = df_features.iloc[inicio_teste:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7e2d23",
      "metadata": {
        "id": "5a7e2d23"
      },
      "source": [
        "### Construir e Compilar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c73dcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "65c73dcf",
        "outputId": "29dfb9c0-9359-42f4-952b-bd35ea944464"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">130,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ p_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ p_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m130,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ p_re_lu_2 (\u001b[38;5;33mPReLU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ p_re_lu_3 (\u001b[38;5;33mPReLU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │            \u001b[38;5;34m32\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,049</span> (554.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m142,049\u001b[0m (554.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,601</span> (553.13 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m141,601\u001b[0m (553.13 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PASSO 3: Construir e compilar o modelo\n",
        "modelo = construir_e_compilar_modelo(SEQUENCE_LENGTH, len(feature_columns), loss_function='binary_focal_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7769e64b",
      "metadata": {
        "id": "7769e64b"
      },
      "source": [
        "### Treinar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e0f0dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25e0f0dc",
        "outputId": "6b4d61db-b126-4329-9653-144f928d4cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m3647/3647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.5061 - loss: 1.5700 - val_accuracy: 0.4983 - val_loss: 1.2150 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m3647/3647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.5076 - loss: 1.1945 - val_accuracy: 0.4983 - val_loss: 1.1596 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m3647/3647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.5066 - loss: 1.1550 - val_accuracy: 0.4983 - val_loss: 1.1539 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m3647/3647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - accuracy: 0.5062 - loss: 1.1521 - val_accuracy: 0.4983 - val_loss: 1.1537 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m3647/3647\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.5039 - loss: 1.1528 - val_accuracy: 0.4983 - val_loss: 1.1535 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m 996/3647\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.5014 - loss: 1.1529"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# PASSO 4: Treinar o modelo\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m historico = \u001b[43mtreinar_modelo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtreinar_modelo\u001b[39m\u001b[34m(model, X_train, y_train, X_test, y_test, class_weight_dict)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Callbacks para otimizar o treinamento\u001b[39;00m\n\u001b[32m      7\u001b[39m callbacks = [\n\u001b[32m      8\u001b[39m     EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m5\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m      9\u001b[39m     ReduceLROnPlateau(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m, min_lr=\u001b[32m0.000001\u001b[39m)\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# epochs: Quantas vezes o modelo verá todo o conjunto de dados de treino.\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch_size: Quantas amostras o modelo vê antes de atualizar seus pesos.\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Dados para validar o modelo a cada época.\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pesos das classes para lidar com desbalanceamento\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Callbacks para otimização do treinamento\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Mostra uma barra de progresso.\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\david\\Documents\\trading-algoritmico\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# PASSO 4: Treinar o modelo\n",
        "historico = treinar_modelo(modelo, X_train, y_train, X_test, y_test, class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18bf3d53",
      "metadata": {
        "id": "18bf3d53"
      },
      "source": [
        "### Avaliar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a163bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50a163bd",
        "outputId": "be9a9b68-2ab7-4878-9839-5cc50147b1b8"
      },
      "outputs": [],
      "source": [
        "# PASSO 5: Avaliar o modelo\n",
        "avaliar_modelo(modelo, X_test, y_test, df_features_teste)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0ce984",
      "metadata": {
        "id": "ec0ce984"
      },
      "source": [
        "### Visualizar o Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4fb688",
      "metadata": {
        "id": "0b4fb688"
      },
      "outputs": [],
      "source": [
        "# PASSO 6: Visualizar o treinamento\n",
        "visualizar_treinamento(historico)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f054563",
      "metadata": {
        "id": "4f054563"
      },
      "source": [
        "### Salvar o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3561a5",
      "metadata": {
        "id": "4a3561a5"
      },
      "outputs": [],
      "source": [
        "# PASSO 7: Salvar o modelo\n",
        "# salvar_modelo_e_scaler(modelo, scaler)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
