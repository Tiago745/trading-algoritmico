{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a546866",
   "metadata": {},
   "source": [
    "# Modelo MLP para Previsão de Preços de Criptomoedas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e45948",
   "metadata": {},
   "source": [
    "## Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03575e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, Normalizer, FunctionTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, PReLU, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, AdamW, Lion, RMSprop\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbfd25f",
   "metadata": {},
   "source": [
    "## PASSO 1: Carregar e Preparar os Dados\n",
    "\n",
    "**Objetivo:** Ler o arquivo CSV e garantir que ele esteja em ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1870108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados do arquivo CSV\n",
    "df = pd.read_csv(r\"..\\..\\data\\fechamentos\\SOLUSDT_5m_data.csv\")\n",
    "\n",
    "# Evita o PC de explodir com muitos dados\n",
    "df = df.iloc[-20000:]\n",
    "\n",
    "# Garante que a coluna 'timestamp' seja do tipo data e ordena os dados\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values('timestamp')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Remove colunas com nomes \"Unnamed\".\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "print(f\"Dados carregados: {df.shape[0]} registros.\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6fb8d",
   "metadata": {},
   "source": [
    "## PASSO 2: Preparar os Dados para o Modelo\n",
    "\n",
    "**Objetivo:** Transformar os dados brutos em um formato que a rede neural entenda (features e target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5584385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculo dos Indicadores ---\n",
    "# Calcula as variações percentuais de 'close' e 'volume'\n",
    "df['close_pct'] = df['close'].pct_change()\n",
    "df['volume_pct'] = df['volume'].pct_change()\n",
    "\n",
    "# Converte close_pct e volume_pct para logaritmo\n",
    "df['close_pct'] = np.log(1 + df['close_pct'])\n",
    "df['volume_pct'] = np.log(1 + df['volume_pct'])\n",
    "\n",
    "# Calcula a média móvel simples de 10 e 30 períodos para 'close'\n",
    "df['sma_10'] = df['close'].rolling(window=10).mean()\n",
    "df['sma_30'] = df['close'].rolling(window=30).mean()\n",
    "\n",
    "# Calcula o Índice de Força Relativa (RSI) de 14 períodos\n",
    "def rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    # Adiciona um pequeno epsilon para evitar divisão por zero\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "df['rsi_7'] = rsi(df['close'], period=7)\n",
    "df['rsi_14'] = rsi(df['close'], period=14)\n",
    "df['rsi_21'] = rsi(df['close'], period=21)\n",
    "\n",
    "# Calcula o MACD\n",
    "def macd(series, short_window=12, long_window=26, signal_window=9):\n",
    "    exp1 = series.ewm(span=short_window, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=long_window, adjust=False).mean()\n",
    "    macd_line = exp1 - exp2\n",
    "    signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()\n",
    "    return macd_line, signal_line\n",
    "df['macd'], df['macd_signal'] = macd(df['close'])\n",
    "\n",
    "# Calcula o Bollinger Bands\n",
    "def bollinger_bands(series, window=20, num_std_dev=2):\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    rolling_std = series.rolling(window=window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * num_std_dev)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std_dev)\n",
    "    return upper_band, lower_band\n",
    "df['bb_upper'], df['bb_lower'] = bollinger_bands(df['close'])\n",
    "\n",
    "# Diferença entre o preço de fechamento e as bandas de Bollinger\n",
    "df['bb_lower_diff'] = df['close'] - df['bb_lower']\n",
    "df['bb_upper_diff'] = df['bb_upper'] - df['close']\n",
    "\n",
    "# Calcula o Average True Range (ATR)\n",
    "def atr(df, window=14):\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_prev_close = abs(df['high'] - df['close'].shift())\n",
    "    low_prev_close = abs(df['low'] - df['close'].shift())\n",
    "    tr = pd.DataFrame({'hl': high_low, 'hpc': high_prev_close, 'lpc': low_prev_close}).max(axis=1)\n",
    "    atr_val = tr.rolling(window=window).mean()\n",
    "    return atr_val\n",
    "df['atr_14'] = atr(df, window=14) # Adicionando ATR de 14 períodos\n",
    "\n",
    "# Calcula o Stochastic Oscillator (%K e %D)\n",
    "def stochastic_oscillator(df, k_window=14, d_window=3):\n",
    "    lowest_low = df['low'].rolling(window=k_window).min()\n",
    "    highest_high = df['high'].rolling(window=k_window).max()\n",
    "    k_percent = ((df['close'] - lowest_low) / (highest_high - lowest_low)) * 100\n",
    "    d_percent = k_percent.rolling(window=d_window).mean()\n",
    "    return k_percent, d_percent\n",
    "df['so_k'] = stochastic_oscillator(df)[0] # %K do Stochastic\n",
    "df['so_d'] = stochastic_oscillator(df)[1] # %D (média de %K) do Stochastic\n",
    "\n",
    "# Calcula o Williams %R\n",
    "def williams_percent_r(df, window=14):\n",
    "    lowest_low = df['low'].rolling(window=window).min()\n",
    "    highest_high = df['high'].rolling(window=window).max()\n",
    "    wpr = ((highest_high - df['close']) / (highest_high - lowest_low)) * -100\n",
    "    return wpr\n",
    "df['wpr_14'] = williams_percent_r(df, window=14) # Williams %R de 14 períodos\n",
    "\n",
    "# Calcula o On-Balance Volume (OBV)\n",
    "def on_balance_volume(df):\n",
    "    obv = pd.Series(0, index=df.index)\n",
    "    for i in range(1, len(df)):\n",
    "        if df['close'].iloc[i] > df['close'].iloc[i-1]:\n",
    "            obv.iloc[i] = obv.iloc[i-1] + df['volume'].iloc[i]\n",
    "        elif df['close'].iloc[i] < df['close'].iloc[i-1]:\n",
    "            obv.iloc[i] = obv.iloc[i-1] - df['volume'].iloc[i]\n",
    "        else:\n",
    "            obv.iloc[i] = obv.iloc[i-1]\n",
    "    return obv\n",
    "df['obv'] = on_balance_volume(df)\n",
    "\n",
    "# Calcula o desvio padrão de 10, 20 e 40 períodos\n",
    "df['std_10'] = df['close'].rolling(window=10).std()\n",
    "df['std_20'] = df['close'].rolling(window=20).std()\n",
    "df['std_40'] = df['close'].rolling(window=40).std()\n",
    "\n",
    "# Calcula a média móvel exponencial de 10 e 30 períodos\n",
    "df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()\n",
    "df['ema_30'] = df['close'].ewm(span=30, adjust=False).mean()\n",
    "\n",
    "# Calcula indicadores lagging\n",
    "df['lag_close_pct_1'] = df['close_pct'].shift(1)  # Preço de fechamento do período anterior\n",
    "df['lag_close_pct_2'] = df['close_pct'].shift(2)  # Preço de fechamento de dois períodos atrás\n",
    "\n",
    "df['lag_rsi_7_1'] = df['rsi_7'].shift(1)  # RSI do período anterior\n",
    "df['lag_rsi_7_2'] = df['rsi_7'].shift(2)  # RSI de dois períodos atrás\n",
    "\n",
    "df['lag_rsi_14_1'] = df['rsi_14'].shift(1)  # RSI de 14 períodos do período anterior\n",
    "df['lag_rsi_14_2'] = df['rsi_14'].shift(2)  # RSI de 14 períodos de dois períodos atrás\n",
    "\n",
    "df['lag_rsi_21_1'] = df['rsi_21'].shift(1)  # RSI de 21 períodos do período anterior\n",
    "df['lag_rsi_21_2'] = df['rsi_21'].shift(2)  # RSI de 21 períodos de dois períodos atrás\n",
    "\n",
    "df['lag_macd_1'] = df['macd'].shift(1)  # MACD do período anterior\n",
    "df['lag_macd_2'] = df['macd'].shift(2)  # MACD de dois períodos atrás\n",
    "\n",
    "df['lag_macd_signal_1'] = df['macd_signal'].shift(1)  # Sinal do MACD do período anterior\n",
    "df['lag_macd_signal_2'] = df['macd_signal'].shift(2)  # Sinal do MACD de dois períodos atrás\n",
    "\n",
    "# Distância do preço atual para a média móvel (mostra se está sobrecomprado/sobrevendido)\n",
    "df['price_vs_sma30'] = df['close'] - df['sma_30']\n",
    "\n",
    "# Cruzamento de médias móveis (um forte indicador de tendência)\n",
    "df['sma_10_vs_sma_30'] = df['sma_10'] - df['sma_30']\n",
    "\n",
    "# Diferença do MACD para a sua linha de sinal\n",
    "df['macd_diff'] = df['macd'] - df['macd_signal']\n",
    "\n",
    "# Volatilidade (desvio padrão dos retornos logarítmicos)\n",
    "df['volatility_10'] = df['close'].pct_change().rolling(window=10).std()\n",
    "df['volatility_20'] = df['close'].pct_change().rolling(window=20).std()\n",
    "df['volatility_40'] = df['close'].pct_change().rolling(window=40).std()\n",
    "\n",
    "# Features de tempo para capturar sazonalidade\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour']/23.0) # Seno da hora\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour']/23.0) # Cosseno da hora\n",
    "df.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# Feature de tendência de volume\n",
    "df['volume_trend'] = df['volume'].pct_change().rolling(window=10).mean()\n",
    "\n",
    "# Feature de tendência de preço\n",
    "df['price_trend'] = df['close'].pct_change().rolling(window=10).mean()\n",
    "\n",
    "# Apaga as linhas com valores NaN\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definição das Variáveis ---\n",
    "FEATURE_COLUMNS = [\n",
    "    'close_pct',          # O último retorno registrado.\n",
    "    'lag_close_pct_1',    # O retorno imediatamente anterior.\n",
    "    'lag_close_pct_2',    # O retorno de dois períodos atrás.\n",
    "    'macd_diff',          # Captura a mudança no momento (momentum).\n",
    "    'atr_14',             # A melhor variável na análise de Informação Mútua.\n",
    "    'volatility_10',      # Medida de volatilidade de curto prazo.\n",
    "    'volatility_20',      # Medida de volatilidade de médio prazo.\n",
    "    'std_10',             # Desvio padrão dos retornos, outra ótima medida.\n",
    "    'bb_lower_diff',      # Distância para a Banda de Bollinger inferior.\n",
    "    'bb_upper_diff',      # Distância para a Banda de Bollinger superior.\n",
    "    'volume',             # Volume bruto, muito importante em ambas as análises.\n",
    "    'obv',                # On-Balance Volume, forte em ambas as análises.\n",
    "    'volume_trend',       # Importante para o modelo.\n",
    "    'price_vs_sma30',     # Onde o preço está em relação à sua média.\n",
    "    'price_trend',        # Tendência de curto prazo do preço.\n",
    "]\n",
    "TARGET_COLUMN = 'close'\n",
    "\n",
    "# Quantos períodos olharemos para trás para prever o próximo\n",
    "SEQUENCE_LENGTH = 36\n",
    "\n",
    "# --- Criação das Features (X) e do Alvo (y) ---\n",
    "# O alvo (y) será binário: 1 se o preço subir, 0 se não\n",
    "# df[TARGET_COLUMN].shift(-1) pega o preço de fechamento do *próximo* período\n",
    "y = (df[TARGET_COLUMN].shift(-1) > df[TARGET_COLUMN]).astype(int)\n",
    "\n",
    "# As features (X) serão os dados da FEATURE_COLUMNS\n",
    "X_raw = df[FEATURE_COLUMNS]\n",
    "\n",
    "num_features = len(FEATURE_COLUMNS)\n",
    "\n",
    "print(f\"Quantidade de features definidas: {num_features}\")\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH}\")\n",
    "print(f\"Shape dos dados brutos: {X_raw.shape}\")\n",
    "print(f\"Valores mínimos {X_raw.min()}\")\n",
    "print(f\"Valores máximos {X_raw.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalização dos Dados ---\n",
    "\n",
    "# Pipeline de normalização: Log transformation + Tanh transformation + Vector normalization\n",
    "\n",
    "# Função personalizada para transformação logaritma\n",
    "def log_transform(X):\n",
    "    \"\"\"Aplica transformação logaritma aos dados\"\"\"\n",
    "    return np.log(X + 1)  # Adiciona 1 para evitar log(0)\n",
    "\n",
    "# Função personalizada para transformação Tanh\n",
    "def tanh_transform(X):\n",
    "    \"\"\"Aplica transformação tanh aos dados para reduzir outliers\"\"\"\n",
    "    return np.tanh(X)\n",
    "\n",
    "# # Criar o pipeline de normalização\n",
    "# scaler = Pipeline([\n",
    "#     # ('log', FunctionTransformer(log_transform, validate=True)),\n",
    "#     ('tanh', FunctionTransformer(tanh_transform, validate=True)),\n",
    "#     ('normalize', Normalizer(norm='l2')),  # Normalização L2 (vetorial)\n",
    "#     # ('minmax', MinMaxScaler(feature_range=(-1, 1)))\n",
    "# ])\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "print(f\"Dados normalizados com Tanh + Vector Normalization. Shape: {X_scaled.shape}\")\n",
    "print(f\"Min: {X_scaled.min():.6f}, Max: {X_scaled.max():.6f}\")\n",
    "# print(f\"Norma L2 das primeiras 5 amostras: {np.linalg.norm(X_scaled[:5], axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6c935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Criação das Sequências ---\n",
    "# Cria sequências de dados para o modelo\n",
    "X_sequences, y_sequences = [], []\n",
    "\n",
    "# Verificar o tamanho correto para evitar index out of bounds\n",
    "max_index = min(len(X_scaled), len(y)) - SEQUENCE_LENGTH\n",
    "\n",
    "print(f\"Tamanho de X_scaled: {len(X_scaled)}\")\n",
    "print(f\"Tamanho de y: {len(y)}\")\n",
    "print(f\"Sequence length: {SEQUENCE_LENGTH}\")\n",
    "print(f\"Max index para loop: {max_index}\")\n",
    "\n",
    "for i in range(max_index):\n",
    "    X_sequences.append(X_scaled[i : i + SEQUENCE_LENGTH])\n",
    "    y_sequences.append(y[i + SEQUENCE_LENGTH - 1]) # O alvo corresponde ao final da sequência\n",
    "\n",
    "X = np.array(X_sequences)\n",
    "y = np.array(y_sequences)\n",
    "\n",
    "print(f\"Sequências criadas. X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Divisão em Treino e Teste ---\n",
    "# Divisão dos dados em treino e teste\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# Tentativa de balancear as classes, evitando a mesma reposta sempre\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Amostras de treino: {len(X_train)}\")\n",
    "print(f\"Amostras de teste: {len(X_test)}\")\n",
    "print(f\"Distribuição do target no treino: {np.bincount(y_train)}\")\n",
    "print(f\"Distribuição do target no teste: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce5f9e",
   "metadata": {},
   "source": [
    "## PASSO 3: Construir o Modelo (MLP)\n",
    "\n",
    "**Objetivo:** Definir a arquitetura da nossa rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b78139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Camada de Entrada: Uso do Conv1D para capturar padrões temporais\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(SEQUENCE_LENGTH, num_features)))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Camada de Pooling: Reduz a dimensionalidade e extrai características importantes\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Achata a saída do Conv1D para ser usada nas camadas densas\n",
    "model.add(Flatten())\n",
    "\n",
    "# 1ª Camada Oculta: Primeira camada que aprende os padrões\n",
    "model.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# PReLU é uma ativação que pode aprender a inclinação\n",
    "model.add(PReLU())\n",
    "\n",
    "# Camada de Dropout: Ajuda a prevenir overfitting\n",
    "# model.add(Dropout(rate=0.5))\n",
    "\n",
    "# 2ª Camada Oculta: Segunda camada para aprender padrões mais complexos\n",
    "model.add(Dense(units=32, kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# PReLU é uma ativação que pode aprender a inclinação\n",
    "model.add(PReLU())\n",
    "\n",
    "# Camada de Dropout: Ajuda a prevenir overfitting\n",
    "# model.add(Dropout(rate=0.3))\n",
    "\n",
    "# Camada de Saída: Gera a previsão final\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ecf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compilação do Modelo ---\n",
    "# O modelo é compilado com o otimizador Adam e a função de perda binary_crossentropy, adequada para problemas de classificação binária\n",
    "model.compile(optimizer=Adam(learning_rate=0.005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Mostra um resumo da arquitetura do modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8796608",
   "metadata": {},
   "source": [
    "## PASSO 4: Treinar o Modelo\n",
    "\n",
    "**Objetivo:** Alimentar o modelo com os dados de treino para que ele aprenda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks para otimizar o treinamento\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.000001)]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=200,  # epochs: Quantas vezes o modelo verá todo o conjunto de dados de treino.\n",
    "    batch_size=16,  # batch_size: Quantas amostras o modelo vê antes de atualizar seus pesos.\n",
    "    validation_data=(X_test, y_test),  # Dados para validar o modelo a cada época.\n",
    "    class_weight=class_weight_dict,  # Pesos das classes para lidar com desbalanceamento\n",
    "    callbacks=callbacks,  # Callbacks para otimização do treinamento\n",
    "    verbose=1 # Mostra uma barra de progresso.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1f5d6",
   "metadata": {},
   "source": [
    "## PASSO 5: Avaliar o Modelo\n",
    "\n",
    "**Objetivo:** Verificar o quão bem o modelo se saiu nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb305fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões nos dados de teste. A saída será uma probabilidade\n",
    "probabilities = model.predict(X_test)\n",
    "\n",
    "# best_accuracy = -np.inf\n",
    "# best_threshold = 0\n",
    "# for threshold in np.arange(0.500, 0.515, 0.0001):\n",
    "#     predictions = (probabilities > threshold).astype(int)\n",
    "#     current_acc = accuracy_score(y_test, predictions)\n",
    "#     if current_acc > best_accuracy:\n",
    "#         best_accuracy = current_acc\n",
    "#         best_threshold = threshold\n",
    "#         print(f\"Melhor acurácia até agora: {best_accuracy:.4f} com threshold {best_threshold:.4f}\")\n",
    "\n",
    "# Converter probabilidades em classes (0 ou 1) usando um limiar de 0.5\n",
    "predictions = (probabilities > 0.5).astype(int)\n",
    "\n",
    "# Calcular e mostrar as métricas\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "report = classification_report(y_test, predictions, target_names=['Não Sobe', 'Sobe'])\n",
    "\n",
    "print(f\"Acurácia no Teste: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMatriz de Confusão:\")\n",
    "# Um mapa de calor para visualizar a matriz de confusão\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Não Sobe', 'Sobe'], yticklabels=['Não Sobe', 'Sobe'])\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma das probabilidades previstas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(probabilities, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Histograma das Probabilidades Previstas')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a3de6",
   "metadata": {},
   "source": [
    "## PASSO 6: Visualizar o Treinamento\n",
    "\n",
    "**Objetivo:** Plotar gráficos para ver se o modelo aprendeu bem ou se teve overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa525db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gráfico da Acurácia\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Acurácia de Treino')\n",
    "plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
    "plt.title('Acurácia ao Longo das Épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico da Perda (Loss)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Perda de Treino')\n",
    "plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
    "plt.title('Perda ao Longo das Épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0065",
   "metadata": {},
   "source": [
    "## PASSO 7: Salvar o Modelo\n",
    "\n",
    "**Objetivo:** Salvar o modelo treinado e o normalizador para uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por enquanto não é necessário salvar o modelo\n",
    "'''\n",
    "MODEL_PATH = \"mlp_basic_model.h5\"\n",
    "SCALER_PATH = \"basic_scaler.pkl\"\n",
    "\n",
    "model.save(MODEL_PATH)\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "\n",
    "print(f\"Modelo salvo em: {MODEL_PATH}\")\n",
    "print(f\"Normalizador salvo em: {SCALER_PATH}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310063f5",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "### Já integrado:\n",
    "\n",
    "1. **Carrega dados históricos**\n",
    "2. **Prepara os dados**\n",
    "3. **Constrói um MLP**\n",
    "4. **Treina o modelo**\n",
    "5. **Avalia o desempenho**\n",
    "6. **Visualiza o treinamento**\n",
    "7. **Salva o modelo**\n",
    "\n",
    "### Próximos Passos:\n",
    "- ENTENDER POR QUE O MODELO SÓ FALA QUE VAI SUBIR (ERA POR CAUSA DO MINMAXSCALER, POR TER MUITOS OUTLIERS ELE APERTAVA VALORES INTERMEDIARIOS EM UM INTERVALO MUITO PEQUENO... TROQUEI PARA STANDARDSCALER)\n",
    "- Adicionar features técnicas, prepara-las e dar ao treino do modelo (não creio que esteja bom ainda)\n",
    "- Implementar validação cruzada temporal (implementei)\n",
    "- Testar diferentes janelas temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30336634",
   "metadata": {},
   "source": [
    "# Testes com outros modelos\n",
    "\n",
    "**Meio freestyle considerando que os dados são reciclados e não adaptados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c101194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com HistGradientBoosting ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# hgd_model = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "# hgd_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# hgd_predictions = hgd_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do HistGradientBoosting:\")\n",
    "# print(classification_report(y_test, hgd_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com RandomForest ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do RandomForest:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com GradientBoosting ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# rf_model = GradientBoostingClassifier(\n",
    "#     n_estimators=200,         # Mais árvores pode melhorar, mas aumenta o tempo de treino\n",
    "#     learning_rate=0.05,       # Menor learning_rate pode melhorar generalização, mas exige mais árvores\n",
    "#     max_depth=4,              # Profundidade das árvores (3-5 é comum)\n",
    "#     subsample=0.8,            # Amostragem estocástica para robustez (0.8 é um bom começo)\n",
    "#     max_features='sqrt',      # Considera sqrt(n_features) em cada split (ajuda a evitar overfitting)\n",
    "#     random_state=42\n",
    "# )\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do GradientBoosting:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49754d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com StackingClassifier ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')),\n",
    "#     ('gb', GradientBoostingClassifier(\n",
    "#         n_estimators=100,\n",
    "#         learning_rate=0.05,\n",
    "#         max_depth=4,\n",
    "#         subsample=0.8,\n",
    "#         max_features='sqrt',\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ]\n",
    "\n",
    "# rf_model = StackingClassifier(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "#     n_jobs=-1,\n",
    "#     passthrough=False\n",
    "# )\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do StackingClassifier:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85430125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# print(\"--- Executando Testes alternativos com ExtraTreesClassifier ---\")\n",
    "\n",
    "# # Crie e treine o modelo\n",
    "# rf_model = ExtraTreesClassifier(\n",
    "#     n_estimators=200,         # Número de árvores na floresta\n",
    "#     max_depth=6,              # Profundidade máxima das árvores (ajuda a evitar overfitting)\n",
    "#     max_features='sqrt',      # Número de features consideradas em cada split\n",
    "#     min_samples_split=5,      # Mínimo de amostras para dividir um nó interno\n",
    "#     min_samples_leaf=3,       # Mínimo de amostras em cada folha\n",
    "#     class_weight='balanced',  # Balanceia as classes automaticamente\n",
    "#     n_jobs=-1,                # Usa todos os núcleos disponíveis\n",
    "#     random_state=42           # Reprodutibilidade\n",
    "# )\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Faça previsões\n",
    "# rf_predictions = rf_model.predict(X_test)\n",
    "# # Avalie o resultado\n",
    "# print(\"\\nRelatório de Classificação do ExtraTreesClassifier:\")\n",
    "# print(classification_report(y_test, rf_predictions, target_names=['Não Sobe', 'Sobe']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
